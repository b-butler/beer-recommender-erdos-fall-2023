{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356263b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import clean\n",
    "import process\n",
    "\n",
    "# import pytorchPipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import train_test_split\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65822b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratings(style=\"range\", threshold=2.5):\n",
    "    \"\"\"Gets the ratings/interaction/user-item matrix\n",
    "    in the form we need to apply the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    style: str, optional\n",
    "        Options: 'range', 'pos_neg', 'zero_one'\n",
    "        The format of the ratings matrix (see process.py)\n",
    "        Defaults to 'range'\n",
    "    threshold: int, optional\n",
    "        The cutoff threshold for \"liking\" a beer; only\n",
    "        matters if style = 'pos_neg' or style = 'zero_one'\n",
    "        (see process.py)\n",
    "        Defaults to 2.5\n",
    "    \"\"\"\n",
    "    if style not in [\"range\", \"pos_neg\", \"zero_one\"]:\n",
    "        raise Exception(\"Not a valid choice for style.\")\n",
    "\n",
    "    reviews = pd.read_csv(\n",
    "        \"https://query.data.world/s/55cb4g2ccy2sbat45jzrwmfjfkp2d5?dws=00000\"\n",
    "    )\n",
    "    reviews.review_time = pd.to_datetime(reviews.review_time, unit=\"s\")\n",
    "    clean_reviews = clean.remove_null_rows(reviews)\n",
    "    clean_reviews = clean.remove_dup_beer_rows(clean_reviews)\n",
    "    clean_reviews = clean.merge_similar_name_breweries(clean_reviews)\n",
    "    clean_reviews = clean.merge_brewery_ids(clean_reviews)\n",
    "\n",
    "    if style == \"range\":\n",
    "        return process.InteractionMatrixTransformer(clean_reviews).to_range().tocsr()\n",
    "    if style == \"pos_neg\":\n",
    "        return (\n",
    "            process.InteractionMatrixTransformer(clean_reviews)\n",
    "            .to_positive_negative(threshold)\n",
    "            .tocsr()\n",
    "        )\n",
    "    if style == \"zero_one\":\n",
    "        return (\n",
    "            process.InteractionMatrixTransformer(clean_reviews)\n",
    "            .to_zero_one(threshold)\n",
    "            .tocsr()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62e8b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(torch.nn.Module):\n",
    "    \"\"\"Matrix factorization model.\n",
    "\n",
    "    - Initializes user and item embeddings (default to dimension 10).\n",
    "    - Forward pass is just a dot product between user and item latent vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_users, num_items, emb_size=10):\n",
    "        super().__init__()\n",
    "        self.user_emb = torch.nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = torch.nn.Embedding(num_items, emb_size)\n",
    "        # initializing weights\n",
    "        self.user_emb.weight.data.uniform_(0, 1)\n",
    "        self.item_emb.weight.data.uniform_(0, 1)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        return (self.user_emb(user) * self.item_emb(item)).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61332b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(model, int_matrix, epochs=10, lr=0.001, wd=0.0):\n",
    "    \"\"\"Trains the matrix factorization model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: MatrixFactorization\n",
    "        The instance of a matrix factorization model to train\n",
    "    int_matrix: scipy.sparse._csr.csr_matrix\n",
    "        The ratings/interaction/user-item matrix\n",
    "    epochs: int, optional\n",
    "        Number of epochs in training\n",
    "        Default is 10\n",
    "    lr: float, optional\n",
    "        Learning rate\n",
    "        Default is 0.001\n",
    "    wd: float, optional\n",
    "        Weight decay\n",
    "        Default is 0.0\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=lr, weight_decay=wd\n",
    "    )  # learning rate\n",
    "    for i in range(epochs):\n",
    "        model.train()  # put model in training mode (?)\n",
    "\n",
    "        # Set gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get mse for nonzero entries\n",
    "        (\n",
    "            rows,\n",
    "            cols,\n",
    "        ) = int_matrix.nonzero()  # (rows[i], cols[i]) across all i will be the coordinates of all nonzero entries of ratings.\n",
    "\n",
    "        pred = model(torch.tensor(rows), torch.tensor(cols))\n",
    "        actual = torch.tensor(\n",
    "            int_matrix[rows, cols], dtype=torch.float32\n",
    "        ).squeeze()  # squeeze just reshapes to the appropriate dim\n",
    "\n",
    "        loss = F.mse_loss(pred, actual)\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"After %.0f epochs, train loss %.10f\" % (i + 1, loss.item()))\n",
    "    print(\"Final RMSE: %.4f\" % loss.item() ** (1 / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2777fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_training(\n",
    "    model, int_matrix, initial_lr=0.01, decay_rate=0.98, epochs=500, wd=0.0\n",
    "):\n",
    "    \"\"\"Standardizes training for matrix factorization\n",
    "    models with learning rate decaying as:\n",
    "    initial_lr * decay_rate^epoch\n",
    "    Stick to the default inputs to standardize training!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: MatrixFactorization\n",
    "        The instance of a matrix factorization model to train\n",
    "    int_matrix: scipy.sparse._csr.csr_matrix\n",
    "        The ratings/interaction/user-item matrix\n",
    "    epochs: int, optional\n",
    "        Number of epochs in training\n",
    "        Default is 10\n",
    "    initial_lr: float, optional\n",
    "        Learning rate\n",
    "        Default is 0.01\n",
    "    decay_rate: float, optional\n",
    "        Decay rate\n",
    "        Default is 1.0\n",
    "    wd: float, optional\n",
    "        Weight decay\n",
    "        Default is 0.0\n",
    "    \"\"\"\n",
    "    model.train()  # put model in training mode (?)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_rate)\n",
    "    for i in range(epochs):\n",
    "        # Set gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get mse for nonzero entries\n",
    "        (\n",
    "            rows,\n",
    "            cols,\n",
    "        ) = int_matrix.nonzero()  # (rows[i], cols[i]) across all i will be the coordinates of all nonzero entries of ratings.\n",
    "\n",
    "        pred = model(torch.tensor(rows), torch.tensor(cols))\n",
    "        actual = torch.tensor(\n",
    "            int_matrix[rows, cols], dtype=torch.float32\n",
    "        ).squeeze()  # squeeze just reshapes to the appropriate dim\n",
    "\n",
    "        loss = F.mse_loss(pred, actual)\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"After %.0f epochs, train loss %.10f\" % (i, loss.item()))\n",
    "    print(\"Final RMSE: %.4f\" % loss.item() ** (1 / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de5e0135",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_matrix = get_ratings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "912d8c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k_range(model, int_matrix, user, threshold=2.5, k=10):\n",
    "    \"\"\"Compute recall at k for a given user in the case\n",
    "    where the ratings matrix was built using to_range.\n",
    "    This is the ratio of number of items the user\n",
    "    actually liked (determined by threshold) in the\n",
    "    top k predictions made by the model over the total\n",
    "    number of items the user actually liked.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    int_matrix: scipy.sparse._csr.csr_matrix\n",
    "        The ratings/interaction/user-item matrix\n",
    "    user: int\n",
    "        The index of the user for whom you want to compute recall at k\n",
    "    threshold: float, optional\n",
    "        The threshold used to determine if a user actually likes a beer\n",
    "        Defaults to 2.5\n",
    "    k: int, optional\n",
    "        The k parameter\n",
    "        Defaults to 10\n",
    "    \"\"\"\n",
    "    num_items = int_matrix.shape[1]\n",
    "    preds = model(\n",
    "        torch.tensor(user), torch.arange(num_items)\n",
    "    )  # predicated ratings of all beers\n",
    "    topk_inds = torch.topk(preds, k).indices  # indices of the top k predictions\n",
    "\n",
    "    # get indices of the liked beers\n",
    "    x = torch.tensor(int_matrix[user].toarray()).squeeze()\n",
    "    inds_of_liked_beers = (\n",
    "        torch.where(x > threshold, 1, 0).squeeze().nonzero()\n",
    "    )  # puts 1 in indices where x > thresh, 0 else then gets the nonzero indices\n",
    "\n",
    "    if inds_of_liked_beers.shape[0] == 0:\n",
    "        # return 'no liked beers'\n",
    "        return 1, 0  # is this how this should go??\n",
    "\n",
    "    intersect = np.intersect1d(topk_inds, inds_of_liked_beers)\n",
    "    # print(len(intersect), len(inds_of_liked_beers))\n",
    "    return len(intersect) / min(k, len(inds_of_liked_beers)), len(inds_of_liked_beers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a3f2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k_nonrange(model, int_matrix, user, k=10):\n",
    "    \"\"\"Compute recall at k for a given user in the case\n",
    "    where the ratings matrix was built using to_zero_one\n",
    "    or to_positive_negative. This is the ratio of number\n",
    "    of items the user actually liked (appear with 1 for\n",
    "    this user in the ratings matrix) in the top k predictions\n",
    "    made by the model over the total number of items the user\n",
    "    actually liked.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: MatrixFactorization\n",
    "        The instance of a matrix factorization model to be evaluated\n",
    "    int_matrix: scipy.sparse._csr.csr_matrix\n",
    "        The ratings/interaction/user-item matrix\n",
    "    user: int\n",
    "        The index of the user for whom you want to compute recall at k\n",
    "    k: int, optional\n",
    "        The k parameter\n",
    "        Defaults to 10\n",
    "    \"\"\"\n",
    "    num_items = int_matrix.shape[1]\n",
    "    preds = model(\n",
    "        torch.tensor(user), torch.arange(num_items)\n",
    "    )  # predicated ratings of all beers\n",
    "    topk_inds = torch.topk(preds, k).indices  # indices of the top k predictions\n",
    "\n",
    "    # get indices of the liked beers\n",
    "    x = torch.tensor(int_matrix[user].toarray())\n",
    "    inds_of_liked_beers = (\n",
    "        torch.where(x == 1, 1, 0).squeeze().nonzero()\n",
    "    )  # puts 1 in indices where x == 1, 0 else then gets the nonzero indices\n",
    "\n",
    "    if inds_of_liked_beers.shape[0] == 0:\n",
    "        # return 'no liked beers'\n",
    "        return 1  # is this how this should be handled??\n",
    "\n",
    "    intersect = np.intersect1d(topk_inds, inds_of_liked_beers)\n",
    "\n",
    "    return len(intersect) / min(\n",
    "        k, len(inds_of_liked_beers)\n",
    "    )  # we use len(inds_of_liked_beers) for the weighted avg part below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4b87d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submatrix(original_matrix, indices):\n",
    "    rows = indices[:, 0]\n",
    "    cols = indices[:, 1]\n",
    "    original_matrix_coo = sp.sparse.coo_matrix(original_matrix)\n",
    "    new_indices = np.where(\n",
    "        (np.isin(original_matrix_coo.row, rows))\n",
    "        & (np.isin(original_matrix_coo.col, cols))\n",
    "    )\n",
    "    return sp.sparse.coo_matrix(\n",
    "        (\n",
    "            original_matrix_coo.data[new_indices],\n",
    "            (\n",
    "                original_matrix_coo.row[new_indices],\n",
    "                original_matrix_coo.col[new_indices],\n",
    "            ),\n",
    "        ),\n",
    "        shape=original_matrix.shape,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5e66cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_with_validation(int_matrix, threshold=2.5, k=10, latent_dim=10):\n",
    "    recall_vals = [0] * 5\n",
    "    preds = [0] * 5\n",
    "    num_users = int_matrix.shape[0]\n",
    "    num_items = int_matrix.shape[1]\n",
    "    i = 0\n",
    "    for train, test in train_test_split.get_splits(int_matrix):\n",
    "        print(\"Pass %d\" % i)\n",
    "\n",
    "        model = MatrixFactorization(num_users, num_items, emb_size=latent_dim)\n",
    "\n",
    "        train_matrix = get_submatrix(int_matrix, train).tocsr()\n",
    "        test_matrix = get_submatrix(int_matrix, test).tocsr()\n",
    "\n",
    "        standard_training(model, train_matrix)\n",
    "        # preds = model(torch.tensor(0), torch.arange(num_items)) # THIS DOESN'T SEEM RIGHT! THIS IS ONLY PREDICTING ON USER 0!\n",
    "        # topk_inds = torch.topk(preds, k).indices\n",
    "        recall = total_weighted_recall_at_k_range(\n",
    "            model, sp.sparse.csr_matrix(test_matrix), threshold=threshold, k=k\n",
    "        )\n",
    "        print(\"Recall: \" + str(recall))\n",
    "        recall_vals[i] = recall\n",
    "        i += 1\n",
    "    return recall_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c036c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_weighted_recall_at_k_range(model, int_matrix, threshold=2.5, k=10):\n",
    "    \"\"\"Compute total weighted recall at k for a given model\n",
    "    in the case where the ratings matrix was built using\n",
    "    to_range. This is the weighted (by total number of liked\n",
    "    beers for a given user) average of the recall_at_k_range\n",
    "    for each user.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: MatrixFactorization\n",
    "        The instance of a matrix factorization model to be evaluated\n",
    "    int_matrix: scipy.sparse._csr.csr_matrix\n",
    "        The ratings/interaction/user-item matrix\n",
    "    threshold: float, optional\n",
    "        The threshold used to determine if a user actually likes a beer\n",
    "        Defaults to 2.5\n",
    "    k: int, optional\n",
    "        The k parameter\n",
    "        Defaults to 10\n",
    "    \"\"\"\n",
    "    num_users, num_items = int_matrix.shape\n",
    "\n",
    "    recalls = torch.empty(num_users)\n",
    "    weights = torch.empty(num_users)\n",
    "\n",
    "    for user in range(num_users):\n",
    "        recall_output = recall_at_k_range(model, int_matrix, user, threshold, k)\n",
    "        recalls[user] = recall_output[0]\n",
    "        weights[user] = recall_output[1]\n",
    "\n",
    "    return (recalls * weights / weights.sum()).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd84532",
   "metadata": {},
   "source": [
    "### Example code for subsetting a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4ae3060",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  4.2 0.  0. ]\n",
      " [2.3 0.  3.1 0. ]\n",
      " [0.  0.  1.5 0. ]\n",
      " [0.  0.  0.  5.7]]\n",
      "[[0.  0.  0.  0. ]\n",
      " [0.  0.  3.1 0. ]\n",
      " [0.  0.  0.  0. ]\n",
      " [0.  0.  0.  5.7]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# Example row, column, and data arrays\n",
    "rows = np.array([0, 1, 2, 1, 3])\n",
    "cols = np.array([1, 2, 2, 0, 3])\n",
    "data = np.array([4.2, 3.1, 1.5, 2.3, 5.7])\n",
    "\n",
    "# Create the original sparse matrix\n",
    "original_matrix = coo_matrix((data, (rows, cols)))\n",
    "print(original_matrix.toarray())\n",
    "# Specify the desired row and column indices for the subset\n",
    "desired_rows = [1, 3]\n",
    "desired_cols = [2, 3]\n",
    "\n",
    "# Find the indices of non-zero elements in the specified rows and columns\n",
    "subset_indices = np.where(\n",
    "    (np.isin(original_matrix.row, desired_rows))\n",
    "    & (np.isin(original_matrix.col, desired_cols))\n",
    ")\n",
    "\n",
    "# Extract the subset of the sparse matrix\n",
    "subset_matrix = coo_matrix(\n",
    "    (\n",
    "        original_matrix.data[subset_indices],\n",
    "        (original_matrix.row[subset_indices], original_matrix.col[subset_indices]),\n",
    "    ),\n",
    "    shape=original_matrix.shape,\n",
    ")\n",
    "\n",
    "print(subset_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9fbf762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(latent_dims):  # latent dims is a list\n",
    "    outs = [[] for _ in range(len(latent_dims))]\n",
    "    # add other hyperparams to gridsearch over?\n",
    "    for i, latent_dim in enumerate(latent_dims):\n",
    "        outs[i].append(latent_dim)\n",
    "        cross_vals = run_model_with_validation(\n",
    "            int_matrix, threshold=2.5, k=10, latent_dim=latent_dim\n",
    "        )\n",
    "        outs[i].append(cross_vals)\n",
    "        outs[i].append(sum(cross_vals) / len(cross_vals))\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "054dd95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass 0\n",
      "After 0 epochs, train loss 7.7158036232\n",
      "After 10 epochs, train loss 5.4535336494\n",
      "After 20 epochs, train loss 3.7970621586\n",
      "After 30 epochs, train loss 2.6976013184\n",
      "After 40 epochs, train loss 2.0144870281\n",
      "After 50 epochs, train loss 1.5988513231\n",
      "After 60 epochs, train loss 1.3413809538\n",
      "After 70 epochs, train loss 1.1757620573\n",
      "After 80 epochs, train loss 1.0647454262\n",
      "After 90 epochs, train loss 0.9875039458\n",
      "After 100 epochs, train loss 0.9320409894\n",
      "After 110 epochs, train loss 0.8911648393\n",
      "After 120 epochs, train loss 0.8603902459\n",
      "After 130 epochs, train loss 0.8368143439\n",
      "After 140 epochs, train loss 0.8184958100\n",
      "After 150 epochs, train loss 0.8040974140\n",
      "After 160 epochs, train loss 0.7926735878\n",
      "After 170 epochs, train loss 0.7835404277\n",
      "After 180 epochs, train loss 0.7761932611\n",
      "After 190 epochs, train loss 0.7702529430\n",
      "After 200 epochs, train loss 0.7654306293\n",
      "After 210 epochs, train loss 0.7615028620\n",
      "After 220 epochs, train loss 0.7582953572\n",
      "After 230 epochs, train loss 0.7556703687\n",
      "After 240 epochs, train loss 0.7535184622\n",
      "After 250 epochs, train loss 0.7517522573\n",
      "After 260 epochs, train loss 0.7503009439\n",
      "After 270 epochs, train loss 0.7491075397\n",
      "After 280 epochs, train loss 0.7481255531\n",
      "After 290 epochs, train loss 0.7473171949\n",
      "After 300 epochs, train loss 0.7466515899\n",
      "After 310 epochs, train loss 0.7461034060\n",
      "After 320 epochs, train loss 0.7456519008\n",
      "After 330 epochs, train loss 0.7452800274\n",
      "After 340 epochs, train loss 0.7449736595\n",
      "After 350 epochs, train loss 0.7447212934\n",
      "After 360 epochs, train loss 0.7445134521\n",
      "After 370 epochs, train loss 0.7443422675\n",
      "After 380 epochs, train loss 0.7442013025\n",
      "After 390 epochs, train loss 0.7440852523\n",
      "After 400 epochs, train loss 0.7439897656\n",
      "After 410 epochs, train loss 0.7439111471\n",
      "After 420 epochs, train loss 0.7438464761\n",
      "After 430 epochs, train loss 0.7437931895\n",
      "After 440 epochs, train loss 0.7437494397\n",
      "After 450 epochs, train loss 0.7437134385\n",
      "After 460 epochs, train loss 0.7436838746\n",
      "After 470 epochs, train loss 0.7436594367\n",
      "After 480 epochs, train loss 0.7436392903\n",
      "After 490 epochs, train loss 0.7436230183\n",
      "Final RMSE: 0.8623\n",
      "Recall: tensor(0.)\n",
      "Pass 1\n",
      "After 0 epochs, train loss 7.7126107216\n",
      "After 10 epochs, train loss 5.4505114555\n",
      "After 20 epochs, train loss 3.7943189144\n",
      "After 30 epochs, train loss 2.6954307556\n",
      "After 40 epochs, train loss 2.0130529404\n",
      "After 50 epochs, train loss 1.5980093479\n",
      "After 60 epochs, train loss 1.3408929110\n",
      "After 70 epochs, train loss 1.1754602194\n",
      "After 80 epochs, train loss 1.0645430088\n",
      "After 90 epochs, train loss 0.9873604178\n",
      "After 100 epochs, train loss 0.9319363832\n",
      "After 110 epochs, train loss 0.8910884857\n",
      "After 120 epochs, train loss 0.8603351712\n",
      "After 130 epochs, train loss 0.8367759585\n",
      "After 140 epochs, train loss 0.8184708953\n",
      "After 150 epochs, train loss 0.8040833473\n",
      "After 160 epochs, train loss 0.7926684618\n",
      "After 170 epochs, train loss 0.7835426927\n",
      "After 180 epochs, train loss 0.7762017250\n",
      "After 190 epochs, train loss 0.7702667117\n",
      "After 200 epochs, train loss 0.7654487491\n",
      "After 210 epochs, train loss 0.7615247369\n",
      "After 220 epochs, train loss 0.7583203316\n",
      "After 230 epochs, train loss 0.7556981444\n",
      "After 240 epochs, train loss 0.7535485625\n",
      "After 250 epochs, train loss 0.7517843843\n",
      "After 260 epochs, train loss 0.7503346801\n",
      "After 270 epochs, train loss 0.7491426468\n",
      "After 280 epochs, train loss 0.7481619120\n",
      "After 290 epochs, train loss 0.7473546267\n",
      "After 300 epochs, train loss 0.7466898561\n",
      "After 310 epochs, train loss 0.7461425066\n",
      "After 320 epochs, train loss 0.7456915379\n",
      "After 330 epochs, train loss 0.7453202009\n",
      "After 340 epochs, train loss 0.7450142503\n",
      "After 350 epochs, train loss 0.7447623014\n",
      "After 360 epochs, train loss 0.7445547581\n",
      "After 370 epochs, train loss 0.7443838716\n",
      "After 380 epochs, train loss 0.7442430854\n",
      "After 390 epochs, train loss 0.7441272736\n",
      "After 400 epochs, train loss 0.7440318465\n",
      "After 410 epochs, train loss 0.7439534068\n",
      "After 420 epochs, train loss 0.7438887954\n",
      "After 430 epochs, train loss 0.7438356876\n",
      "After 440 epochs, train loss 0.7437918782\n",
      "After 450 epochs, train loss 0.7437559366\n",
      "After 460 epochs, train loss 0.7437264323\n",
      "After 470 epochs, train loss 0.7437020540\n",
      "After 480 epochs, train loss 0.7436820865\n",
      "After 490 epochs, train loss 0.7436656356\n",
      "Final RMSE: 0.8624\n",
      "Recall: tensor(0.)\n",
      "Pass 2\n",
      "After 0 epochs, train loss 7.7073860168\n",
      "After 10 epochs, train loss 5.4464106560\n",
      "After 20 epochs, train loss 3.7917718887\n",
      "After 30 epochs, train loss 2.6942036152\n",
      "After 40 epochs, train loss 2.0127034187\n",
      "After 50 epochs, train loss 1.5982315540\n",
      "After 60 epochs, train loss 1.3414769173\n",
      "After 70 epochs, train loss 1.1762521267\n",
      "After 80 epochs, train loss 1.0654453039\n",
      "After 90 epochs, train loss 0.9883192182\n",
      "After 100 epochs, train loss 0.9329238534\n",
      "After 110 epochs, train loss 0.8920906186\n",
      "After 120 epochs, train loss 0.8613448143\n",
      "After 130 epochs, train loss 0.8377894759\n",
      "After 140 epochs, train loss 0.8194861412\n",
      "After 150 epochs, train loss 0.8050994277\n",
      "After 160 epochs, train loss 0.7936848402\n",
      "After 170 epochs, train loss 0.7845590115\n",
      "After 180 epochs, train loss 0.7772179842\n",
      "After 190 epochs, train loss 0.7712827921\n",
      "After 200 epochs, train loss 0.7664646506\n",
      "After 210 epochs, train loss 0.7625405192\n",
      "After 220 epochs, train loss 0.7593361139\n",
      "After 230 epochs, train loss 0.7567136288\n",
      "After 240 epochs, train loss 0.7545641065\n",
      "After 250 epochs, train loss 0.7527996898\n",
      "After 260 epochs, train loss 0.7513500452\n",
      "After 270 epochs, train loss 0.7501580119\n",
      "After 280 epochs, train loss 0.7491771579\n",
      "After 290 epochs, train loss 0.7483699322\n",
      "After 300 epochs, train loss 0.7477052212\n",
      "After 310 epochs, train loss 0.7471577525\n",
      "After 320 epochs, train loss 0.7467068434\n",
      "After 330 epochs, train loss 0.7463355064\n",
      "After 340 epochs, train loss 0.7460294962\n",
      "After 350 epochs, train loss 0.7457775474\n",
      "After 360 epochs, train loss 0.7455700040\n",
      "After 370 epochs, train loss 0.7453991771\n",
      "After 380 epochs, train loss 0.7452584505\n",
      "After 390 epochs, train loss 0.7451425791\n",
      "After 400 epochs, train loss 0.7450472116\n",
      "After 410 epochs, train loss 0.7449686527\n",
      "After 420 epochs, train loss 0.7449041009\n",
      "After 430 epochs, train loss 0.7448508739\n",
      "After 440 epochs, train loss 0.7448071837\n",
      "After 450 epochs, train loss 0.7447713017\n",
      "After 460 epochs, train loss 0.7447416186\n",
      "After 470 epochs, train loss 0.7447173595\n",
      "After 480 epochs, train loss 0.7446973920\n",
      "After 490 epochs, train loss 0.7446810007\n",
      "Final RMSE: 0.8629\n",
      "Recall: tensor(0.0002)\n",
      "Pass 3\n",
      "After 0 epochs, train loss 7.7396016121\n",
      "After 10 epochs, train loss 5.4771575928\n",
      "After 20 epochs, train loss 3.8183410168\n",
      "After 30 epochs, train loss 2.7152583599\n",
      "After 40 epochs, train loss 2.0286612511\n",
      "After 50 epochs, train loss 1.6102584600\n",
      "After 60 epochs, train loss 1.3506646156\n",
      "After 70 epochs, train loss 1.1833958626\n",
      "After 80 epochs, train loss 1.0710933208\n",
      "After 90 epochs, train loss 0.9928530455\n",
      "After 100 epochs, train loss 0.9366161823\n",
      "After 110 epochs, train loss 0.8951397538\n",
      "After 120 epochs, train loss 0.8638967872\n",
      "After 130 epochs, train loss 0.8399533629\n",
      "After 140 epochs, train loss 0.8213444352\n",
      "After 150 epochs, train loss 0.8067149520\n",
      "After 160 epochs, train loss 0.7951061726\n",
      "After 170 epochs, train loss 0.7858242989\n",
      "After 180 epochs, train loss 0.7783568501\n",
      "After 190 epochs, train loss 0.7723190188\n",
      "After 200 epochs, train loss 0.7674173713\n",
      "After 210 epochs, train loss 0.7634248137\n",
      "After 220 epochs, train loss 0.7601642013\n",
      "After 230 epochs, train loss 0.7574959993\n",
      "After 240 epochs, train loss 0.7553085685\n",
      "After 250 epochs, train loss 0.7535131574\n",
      "After 260 epochs, train loss 0.7520377636\n",
      "After 270 epochs, train loss 0.7508246303\n",
      "After 280 epochs, train loss 0.7498264313\n",
      "After 290 epochs, train loss 0.7490048409\n",
      "After 300 epochs, train loss 0.7483282685\n",
      "After 310 epochs, train loss 0.7477710247\n",
      "After 320 epochs, train loss 0.7473121285\n",
      "After 330 epochs, train loss 0.7469339967\n",
      "After 340 epochs, train loss 0.7466226220\n",
      "After 350 epochs, train loss 0.7463662028\n",
      "After 360 epochs, train loss 0.7461549044\n",
      "After 370 epochs, train loss 0.7459809780\n",
      "After 380 epochs, train loss 0.7458376884\n",
      "After 390 epochs, train loss 0.7457197905\n",
      "After 400 epochs, train loss 0.7456226349\n",
      "After 410 epochs, train loss 0.7455428243\n",
      "After 420 epochs, train loss 0.7454769611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 430 epochs, train loss 0.7454228997\n",
      "After 440 epochs, train loss 0.7453783751\n",
      "After 450 epochs, train loss 0.7453418374\n",
      "After 460 epochs, train loss 0.7453116179\n",
      "After 470 epochs, train loss 0.7452869415\n",
      "After 480 epochs, train loss 0.7452666163\n",
      "After 490 epochs, train loss 0.7452499270\n",
      "Final RMSE: 0.8633\n",
      "Recall: tensor(0.)\n",
      "Pass 4\n",
      "After 0 epochs, train loss 7.7116580009\n",
      "After 10 epochs, train loss 5.4492239952\n",
      "After 20 epochs, train loss 3.7928190231\n",
      "After 30 epochs, train loss 2.6935372353\n",
      "After 40 epochs, train loss 2.0110001564\n",
      "After 50 epochs, train loss 1.5961890221\n",
      "After 60 epochs, train loss 1.3395183086\n",
      "After 70 epochs, train loss 1.1745560169\n",
      "After 80 epochs, train loss 1.0640444756\n",
      "After 90 epochs, train loss 0.9871822596\n",
      "After 100 epochs, train loss 0.9320022464\n",
      "After 110 epochs, train loss 0.8913378119\n",
      "After 120 epochs, train loss 0.8607226014\n",
      "After 130 epochs, train loss 0.8372677565\n",
      "After 140 epochs, train loss 0.8190422654\n",
      "After 150 epochs, train loss 0.8047160506\n",
      "After 160 epochs, train loss 0.7933487296\n",
      "After 170 epochs, train loss 0.7842603922\n",
      "After 180 epochs, train loss 0.7769488692\n",
      "After 190 epochs, train loss 0.7710372806\n",
      "After 200 epochs, train loss 0.7662381530\n",
      "After 210 epochs, train loss 0.7623292804\n",
      "After 220 epochs, train loss 0.7591370344\n",
      "After 230 epochs, train loss 0.7565248013\n",
      "After 240 epochs, train loss 0.7543833852\n",
      "After 250 epochs, train loss 0.7526255846\n",
      "After 260 epochs, train loss 0.7511813641\n",
      "After 270 epochs, train loss 0.7499937415\n",
      "After 280 epochs, train loss 0.7490165830\n",
      "After 290 epochs, train loss 0.7482122183\n",
      "After 300 epochs, train loss 0.7475500107\n",
      "After 310 epochs, train loss 0.7470045090\n",
      "After 320 epochs, train loss 0.7465553284\n",
      "After 330 epochs, train loss 0.7461852431\n",
      "After 340 epochs, train loss 0.7458804250\n",
      "After 350 epochs, train loss 0.7456293106\n",
      "After 360 epochs, train loss 0.7454225421\n",
      "After 370 epochs, train loss 0.7452523112\n",
      "After 380 epochs, train loss 0.7451120615\n",
      "After 390 epochs, train loss 0.7449966669\n",
      "After 400 epochs, train loss 0.7449015379\n",
      "After 410 epochs, train loss 0.7448233366\n",
      "After 420 epochs, train loss 0.7447590828\n",
      "After 430 epochs, train loss 0.7447060347\n",
      "After 440 epochs, train loss 0.7446624637\n",
      "After 450 epochs, train loss 0.7446266413\n",
      "After 460 epochs, train loss 0.7445971966\n",
      "After 470 epochs, train loss 0.7445730567\n",
      "After 480 epochs, train loss 0.7445530295\n",
      "After 490 epochs, train loss 0.7445366979\n",
      "Final RMSE: 0.8629\n",
      "Recall: tensor(0.)\n",
      "Pass 0\n",
      "After 0 epochs, train loss 2.9914414883\n",
      "After 10 epochs, train loss 1.3249094486\n",
      "After 20 epochs, train loss 0.8139067292\n",
      "After 30 epochs, train loss 0.6426110268\n",
      "After 40 epochs, train loss 0.5756280422\n",
      "After 50 epochs, train loss 0.5461302996\n",
      "After 60 epochs, train loss 0.5308564901\n",
      "After 70 epochs, train loss 0.5220037699\n",
      "After 80 epochs, train loss 0.5162745118\n",
      "After 90 epochs, train loss 0.5122522712\n",
      "After 100 epochs, train loss 0.5092749000\n",
      "After 110 epochs, train loss 0.5069917440\n",
      "After 120 epochs, train loss 0.5051996708\n",
      "After 130 epochs, train loss 0.5037708879\n",
      "After 140 epochs, train loss 0.5026196837\n",
      "After 150 epochs, train loss 0.5016850829\n",
      "After 160 epochs, train loss 0.5009223819\n",
      "After 170 epochs, train loss 0.5002973676\n",
      "After 180 epochs, train loss 0.4997836947\n",
      "After 190 epochs, train loss 0.4993605614\n",
      "After 200 epochs, train loss 0.4990114272\n",
      "After 210 epochs, train loss 0.4987231195\n",
      "After 220 epochs, train loss 0.4984847009\n",
      "After 230 epochs, train loss 0.4982874691\n",
      "After 240 epochs, train loss 0.4981242716\n",
      "After 250 epochs, train loss 0.4979891479\n",
      "After 260 epochs, train loss 0.4978772700\n",
      "After 270 epochs, train loss 0.4977845550\n",
      "After 280 epochs, train loss 0.4977079034\n",
      "After 290 epochs, train loss 0.4976443648\n",
      "After 300 epochs, train loss 0.4975917339\n",
      "After 310 epochs, train loss 0.4975482821\n",
      "After 320 epochs, train loss 0.4975122213\n",
      "After 330 epochs, train loss 0.4974824190\n",
      "After 340 epochs, train loss 0.4974577725\n",
      "After 350 epochs, train loss 0.4974373579\n",
      "After 360 epochs, train loss 0.4974204898\n",
      "After 370 epochs, train loss 0.4974065423\n",
      "After 380 epochs, train loss 0.4973950684\n",
      "After 390 epochs, train loss 0.4973855615\n",
      "After 400 epochs, train loss 0.4973776937\n",
      "After 410 epochs, train loss 0.4973711967\n",
      "After 420 epochs, train loss 0.4973658323\n",
      "After 430 epochs, train loss 0.4973614514\n",
      "After 440 epochs, train loss 0.4973577261\n",
      "After 450 epochs, train loss 0.4973547757\n",
      "After 460 epochs, train loss 0.4973522425\n",
      "After 470 epochs, train loss 0.4973502457\n",
      "After 480 epochs, train loss 0.4973485470\n",
      "After 490 epochs, train loss 0.4973471463\n",
      "Final RMSE: 0.7052\n",
      "Recall: tensor(0.)\n",
      "Pass 1\n",
      "After 0 epochs, train loss 2.9944806099\n",
      "After 10 epochs, train loss 1.3265556097\n",
      "After 20 epochs, train loss 0.8138243556\n",
      "After 30 epochs, train loss 0.6406920552\n",
      "After 40 epochs, train loss 0.5744763017\n",
      "After 50 epochs, train loss 0.5454422832\n",
      "After 60 epochs, train loss 0.5303422809\n",
      "After 70 epochs, train loss 0.5215787888\n",
      "After 80 epochs, train loss 0.5159049034\n",
      "After 90 epochs, train loss 0.5119160414\n",
      "After 100 epochs, train loss 0.5089586377\n",
      "After 110 epochs, train loss 0.5066881776\n",
      "After 120 epochs, train loss 0.5049043894\n",
      "After 130 epochs, train loss 0.5034814477\n",
      "After 140 epochs, train loss 0.5023343563\n",
      "After 150 epochs, train loss 0.5014028549\n",
      "After 160 epochs, train loss 0.5006422997\n",
      "After 170 epochs, train loss 0.5000189543\n",
      "After 180 epochs, train loss 0.4995065928\n",
      "After 190 epochs, train loss 0.4990844131\n",
      "After 200 epochs, train loss 0.4987361729\n",
      "After 210 epochs, train loss 0.4984484911\n",
      "After 220 epochs, train loss 0.4982105494\n",
      "After 230 epochs, train loss 0.4980136752\n",
      "After 240 epochs, train loss 0.4978507757\n",
      "After 250 epochs, train loss 0.4977158904\n",
      "After 260 epochs, train loss 0.4976042211\n",
      "After 270 epochs, train loss 0.4975117743\n",
      "After 280 epochs, train loss 0.4974351823\n",
      "After 290 epochs, train loss 0.4973717928\n",
      "After 300 epochs, train loss 0.4973192215\n",
      "After 310 epochs, train loss 0.4972757995\n",
      "After 320 epochs, train loss 0.4972398877\n",
      "After 330 epochs, train loss 0.4972101152\n",
      "After 340 epochs, train loss 0.4971854985\n",
      "After 350 epochs, train loss 0.4971651137\n",
      "After 360 epochs, train loss 0.4971483052\n",
      "After 370 epochs, train loss 0.4971343875\n",
      "After 380 epochs, train loss 0.4971228838\n",
      "After 390 epochs, train loss 0.4971134067\n",
      "After 400 epochs, train loss 0.4971055388\n",
      "After 410 epochs, train loss 0.4970990717\n",
      "After 420 epochs, train loss 0.4970937073\n",
      "After 430 epochs, train loss 0.4970893264\n",
      "After 440 epochs, train loss 0.4970856905\n",
      "After 450 epochs, train loss 0.4970826507\n",
      "After 460 epochs, train loss 0.4970801473\n",
      "After 470 epochs, train loss 0.4970781505\n",
      "After 480 epochs, train loss 0.4970764518\n",
      "After 490 epochs, train loss 0.4970750213\n",
      "Final RMSE: 0.7050\n",
      "Recall: tensor(0.)\n",
      "Pass 2\n",
      "After 0 epochs, train loss 3.0125401020\n",
      "After 10 epochs, train loss 1.3422613144\n",
      "After 20 epochs, train loss 0.8217316866\n",
      "After 30 epochs, train loss 0.6426388025\n",
      "After 40 epochs, train loss 0.5740714669\n",
      "After 50 epochs, train loss 0.5443269610\n",
      "After 60 epochs, train loss 0.5291109681\n",
      "After 70 epochs, train loss 0.5203553438\n",
      "After 80 epochs, train loss 0.5147239566\n",
      "After 90 epochs, train loss 0.5107875466\n",
      "After 100 epochs, train loss 0.5078786016\n",
      "After 110 epochs, train loss 0.5056490302\n",
      "After 120 epochs, train loss 0.5038991570\n",
      "After 130 epochs, train loss 0.5025040507\n",
      "After 140 epochs, train loss 0.5013798475\n",
      "After 150 epochs, train loss 0.5004670620\n",
      "After 160 epochs, train loss 0.4997219443\n",
      "After 170 epochs, train loss 0.4991113245\n",
      "After 180 epochs, train loss 0.4986094236\n",
      "After 190 epochs, train loss 0.4981959164\n",
      "After 200 epochs, train loss 0.4978547692\n",
      "After 210 epochs, train loss 0.4975729287\n",
      "After 220 epochs, train loss 0.4973399341\n",
      "After 230 epochs, train loss 0.4971471429\n",
      "After 240 epochs, train loss 0.4969875515\n",
      "After 250 epochs, train loss 0.4968554974\n",
      "After 260 epochs, train loss 0.4967461526\n",
      "After 270 epochs, train loss 0.4966556132\n",
      "After 280 epochs, train loss 0.4965805709\n",
      "After 290 epochs, train loss 0.4965184629\n",
      "After 300 epochs, train loss 0.4964670837\n",
      "After 310 epochs, train loss 0.4964245260\n",
      "After 320 epochs, train loss 0.4963893294\n",
      "After 330 epochs, train loss 0.4963602126\n",
      "After 340 epochs, train loss 0.4963361323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 350 epochs, train loss 0.4963162243\n",
      "After 360 epochs, train loss 0.4962997139\n",
      "After 370 epochs, train loss 0.4962860942\n",
      "After 380 epochs, train loss 0.4962748289\n",
      "After 390 epochs, train loss 0.4962655604\n",
      "After 400 epochs, train loss 0.4962579310\n",
      "After 410 epochs, train loss 0.4962514937\n",
      "After 420 epochs, train loss 0.4962462187\n",
      "After 430 epochs, train loss 0.4962419868\n",
      "After 440 epochs, train loss 0.4962383509\n",
      "After 450 epochs, train loss 0.4962354302\n",
      "After 460 epochs, train loss 0.4962329865\n",
      "After 470 epochs, train loss 0.4962310195\n",
      "After 480 epochs, train loss 0.4962293208\n",
      "After 490 epochs, train loss 0.4962280095\n",
      "Final RMSE: 0.7044\n",
      "Recall: tensor(0.)\n",
      "Pass 3\n",
      "After 0 epochs, train loss 3.0246081352\n",
      "After 10 epochs, train loss 1.3459948301\n",
      "After 20 epochs, train loss 0.8257319331\n",
      "After 30 epochs, train loss 0.6457535625\n",
      "After 40 epochs, train loss 0.5760325193\n",
      "After 50 epochs, train loss 0.5457618833\n",
      "After 60 epochs, train loss 0.5302664042\n",
      "After 70 epochs, train loss 0.5213881135\n",
      "After 80 epochs, train loss 0.5156931281\n",
      "After 90 epochs, train loss 0.5117167234\n",
      "After 100 epochs, train loss 0.5087813735\n",
      "After 110 epochs, train loss 0.5065336227\n",
      "After 120 epochs, train loss 0.5047705770\n",
      "After 130 epochs, train loss 0.5033655167\n",
      "After 140 epochs, train loss 0.5022336841\n",
      "After 150 epochs, train loss 0.5013148189\n",
      "After 160 epochs, train loss 0.5005649924\n",
      "After 170 epochs, train loss 0.4999504089\n",
      "After 180 epochs, train loss 0.4994453490\n",
      "After 190 epochs, train loss 0.4990292788\n",
      "After 200 epochs, train loss 0.4986859858\n",
      "After 210 epochs, train loss 0.4984023571\n",
      "After 220 epochs, train loss 0.4981679022\n",
      "After 230 epochs, train loss 0.4979739189\n",
      "After 240 epochs, train loss 0.4978132844\n",
      "After 250 epochs, train loss 0.4976803958\n",
      "After 260 epochs, train loss 0.4975703061\n",
      "After 270 epochs, train loss 0.4974791706\n",
      "After 280 epochs, train loss 0.4974036813\n",
      "After 290 epochs, train loss 0.4973411560\n",
      "After 300 epochs, train loss 0.4972894490\n",
      "After 310 epochs, train loss 0.4972466230\n",
      "After 320 epochs, train loss 0.4972111285\n",
      "After 330 epochs, train loss 0.4971818030\n",
      "After 340 epochs, train loss 0.4971576035\n",
      "After 350 epochs, train loss 0.4971375763\n",
      "After 360 epochs, train loss 0.4971209168\n",
      "After 370 epochs, train loss 0.4971072376\n",
      "After 380 epochs, train loss 0.4970958829\n",
      "After 390 epochs, train loss 0.4970864952\n",
      "After 400 epochs, train loss 0.4970788062\n",
      "After 410 epochs, train loss 0.4970723987\n",
      "After 420 epochs, train loss 0.4970671535\n",
      "After 430 epochs, train loss 0.4970627725\n",
      "After 440 epochs, train loss 0.4970591664\n",
      "After 450 epochs, train loss 0.4970562160\n",
      "After 460 epochs, train loss 0.4970537722\n",
      "After 470 epochs, train loss 0.4970517159\n",
      "After 480 epochs, train loss 0.4970501065\n",
      "After 490 epochs, train loss 0.4970486760\n",
      "Final RMSE: 0.7050\n",
      "Recall: tensor(0.)\n",
      "Pass 4\n",
      "After 0 epochs, train loss 3.0677270889\n",
      "After 10 epochs, train loss 1.3719021082\n",
      "After 20 epochs, train loss 0.8421617746\n",
      "After 30 epochs, train loss 0.6569934487\n",
      "After 40 epochs, train loss 0.5841047168\n",
      "After 50 epochs, train loss 0.5518045425\n",
      "After 60 epochs, train loss 0.5349457264\n",
      "After 70 epochs, train loss 0.5251833797\n",
      "After 80 epochs, train loss 0.5188891888\n",
      "After 90 epochs, train loss 0.5144938827\n",
      "After 100 epochs, train loss 0.5112580657\n",
      "After 110 epochs, train loss 0.5087890029\n",
      "After 120 epochs, train loss 0.5068594217\n",
      "After 130 epochs, train loss 0.5053268075\n",
      "After 140 epochs, train loss 0.5040958524\n",
      "After 150 epochs, train loss 0.5030992031\n",
      "After 160 epochs, train loss 0.5022875071\n",
      "After 170 epochs, train loss 0.5016234517\n",
      "After 180 epochs, train loss 0.5010784268\n",
      "After 190 epochs, train loss 0.5006301403\n",
      "After 200 epochs, train loss 0.5002605319\n",
      "After 210 epochs, train loss 0.4999554753\n",
      "After 220 epochs, train loss 0.4997035265\n",
      "After 230 epochs, train loss 0.4994950891\n",
      "After 240 epochs, train loss 0.4993226826\n",
      "After 250 epochs, train loss 0.4991799891\n",
      "After 260 epochs, train loss 0.4990618825\n",
      "After 270 epochs, train loss 0.4989640713\n",
      "After 280 epochs, train loss 0.4988831282\n",
      "After 290 epochs, train loss 0.4988161027\n",
      "After 300 epochs, train loss 0.4987606406\n",
      "After 310 epochs, train loss 0.4987147450\n",
      "After 320 epochs, train loss 0.4986767173\n",
      "After 330 epochs, train loss 0.4986452460\n",
      "After 340 epochs, train loss 0.4986192882\n",
      "After 350 epochs, train loss 0.4985978305\n",
      "After 360 epochs, train loss 0.4985800385\n",
      "After 370 epochs, train loss 0.4985652566\n",
      "After 380 epochs, train loss 0.4985531569\n",
      "After 390 epochs, train loss 0.4985430837\n",
      "After 400 epochs, train loss 0.4985348284\n",
      "After 410 epochs, train loss 0.4985279441\n",
      "After 420 epochs, train loss 0.4985223413\n",
      "After 430 epochs, train loss 0.4985176623\n",
      "After 440 epochs, train loss 0.4985137880\n",
      "After 450 epochs, train loss 0.4985105991\n",
      "After 460 epochs, train loss 0.4985080063\n",
      "After 470 epochs, train loss 0.4985058606\n",
      "After 480 epochs, train loss 0.4985040426\n",
      "After 490 epochs, train loss 0.4985025823\n",
      "Final RMSE: 0.7060\n",
      "Recall: tensor(0.)\n",
      "Pass 0\n",
      "After 0 epochs, train loss 3.0125017166\n",
      "After 10 epochs, train loss 0.9563519359\n",
      "After 20 epochs, train loss 0.7008335590\n",
      "After 30 epochs, train loss 0.6071333289\n",
      "After 40 epochs, train loss 0.5647895336\n",
      "After 50 epochs, train loss 0.5424600244\n",
      "After 60 epochs, train loss 0.5289162397\n",
      "After 70 epochs, train loss 0.5197525024\n",
      "After 80 epochs, train loss 0.5130663514\n",
      "After 90 epochs, train loss 0.5079534650\n",
      "After 100 epochs, train loss 0.5039374232\n",
      "After 110 epochs, train loss 0.5007290244\n",
      "After 120 epochs, train loss 0.4981377423\n",
      "After 130 epochs, train loss 0.4960295260\n",
      "After 140 epochs, train loss 0.4943054616\n",
      "After 150 epochs, train loss 0.4928901792\n",
      "After 160 epochs, train loss 0.4917251170\n",
      "After 170 epochs, train loss 0.4907640517\n",
      "After 180 epochs, train loss 0.4899700582\n",
      "After 190 epochs, train loss 0.4893132746\n",
      "After 200 epochs, train loss 0.4887695611\n",
      "After 210 epochs, train loss 0.4883191884\n",
      "After 220 epochs, train loss 0.4879460037\n",
      "After 230 epochs, train loss 0.4876366854\n",
      "After 240 epochs, train loss 0.4873802662\n",
      "After 250 epochs, train loss 0.4871677160\n",
      "After 260 epochs, train loss 0.4869915843\n",
      "After 270 epochs, train loss 0.4868455529\n",
      "After 280 epochs, train loss 0.4867245853\n",
      "After 290 epochs, train loss 0.4866243303\n",
      "After 300 epochs, train loss 0.4865413308\n",
      "After 310 epochs, train loss 0.4864725471\n",
      "After 320 epochs, train loss 0.4864156544\n",
      "After 330 epochs, train loss 0.4863685369\n",
      "After 340 epochs, train loss 0.4863295853\n",
      "After 350 epochs, train loss 0.4862973392\n",
      "After 360 epochs, train loss 0.4862706065\n",
      "After 370 epochs, train loss 0.4862486422\n",
      "After 380 epochs, train loss 0.4862303138\n",
      "After 390 epochs, train loss 0.4862152934\n",
      "After 400 epochs, train loss 0.4862029254\n",
      "After 410 epochs, train loss 0.4861926436\n",
      "After 420 epochs, train loss 0.4861841500\n",
      "After 430 epochs, train loss 0.4861771464\n",
      "After 440 epochs, train loss 0.4861713648\n",
      "After 450 epochs, train loss 0.4861665964\n",
      "After 460 epochs, train loss 0.4861626327\n",
      "After 470 epochs, train loss 0.4861593544\n",
      "After 480 epochs, train loss 0.4861567020\n",
      "After 490 epochs, train loss 0.4861545265\n",
      "Final RMSE: 0.6972\n",
      "Recall: tensor(4.7285e-05)\n",
      "Pass 1\n",
      "After 0 epochs, train loss 3.0411603451\n",
      "After 10 epochs, train loss 0.9709365368\n",
      "After 20 epochs, train loss 0.7046230435\n",
      "After 30 epochs, train loss 0.6079354882\n",
      "After 40 epochs, train loss 0.5647364259\n",
      "After 50 epochs, train loss 0.5421066284\n",
      "After 60 epochs, train loss 0.5285397172\n",
      "After 70 epochs, train loss 0.5194054842\n",
      "After 80 epochs, train loss 0.5127633214\n",
      "After 90 epochs, train loss 0.5076966286\n",
      "After 100 epochs, train loss 0.5037210584\n",
      "After 110 epochs, train loss 0.5005470514\n",
      "After 120 epochs, train loss 0.4979846179\n",
      "After 130 epochs, train loss 0.4959002733\n",
      "After 140 epochs, train loss 0.4941958785\n",
      "After 150 epochs, train loss 0.4927967787\n",
      "After 160 epochs, train loss 0.4916451573\n",
      "After 170 epochs, train loss 0.4906951487\n",
      "After 180 epochs, train loss 0.4899102151\n",
      "After 190 epochs, train loss 0.4892610013\n",
      "After 200 epochs, train loss 0.4887235165\n",
      "After 210 epochs, train loss 0.4882782698\n",
      "After 220 epochs, train loss 0.4879093468\n",
      "After 230 epochs, train loss 0.4876035154\n",
      "After 240 epochs, train loss 0.4873500466\n",
      "After 250 epochs, train loss 0.4871399403\n",
      "After 260 epochs, train loss 0.4869657755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 270 epochs, train loss 0.4868214130\n",
      "After 280 epochs, train loss 0.4867017567\n",
      "After 290 epochs, train loss 0.4866026938\n",
      "After 300 epochs, train loss 0.4865205884\n",
      "After 310 epochs, train loss 0.4864526093\n",
      "After 320 epochs, train loss 0.4863963127\n",
      "After 330 epochs, train loss 0.4863497615\n",
      "After 340 epochs, train loss 0.4863111973\n",
      "After 350 epochs, train loss 0.4862793088\n",
      "After 360 epochs, train loss 0.4862529933\n",
      "After 370 epochs, train loss 0.4862311780\n",
      "After 380 epochs, train loss 0.4862131476\n",
      "After 390 epochs, train loss 0.4861982465\n",
      "After 400 epochs, train loss 0.4861859083\n",
      "After 410 epochs, train loss 0.4861757457\n",
      "After 420 epochs, train loss 0.4861674309\n",
      "After 430 epochs, train loss 0.4861605167\n",
      "After 440 epochs, train loss 0.4861547649\n",
      "After 450 epochs, train loss 0.4861500561\n",
      "After 460 epochs, train loss 0.4861461520\n",
      "After 470 epochs, train loss 0.4861429632\n",
      "After 480 epochs, train loss 0.4861403108\n",
      "After 490 epochs, train loss 0.4861381054\n",
      "Final RMSE: 0.6972\n",
      "Recall: tensor(0.)\n",
      "Pass 2\n",
      "After 0 epochs, train loss 3.0184271336\n",
      "After 10 epochs, train loss 0.9679612517\n",
      "After 20 epochs, train loss 0.7002314925\n",
      "After 30 epochs, train loss 0.6063964963\n",
      "After 40 epochs, train loss 0.5641599894\n",
      "After 50 epochs, train loss 0.5416993499\n",
      "After 60 epochs, train loss 0.5280851126\n",
      "After 70 epochs, train loss 0.5188889503\n",
      "After 80 epochs, train loss 0.5121886730\n",
      "After 90 epochs, train loss 0.5070727468\n",
      "After 100 epochs, train loss 0.5030560493\n",
      "After 110 epochs, train loss 0.4998480380\n",
      "After 120 epochs, train loss 0.4972577691\n",
      "After 130 epochs, train loss 0.4951507747\n",
      "After 140 epochs, train loss 0.4934279621\n",
      "After 150 epochs, train loss 0.4920138717\n",
      "After 160 epochs, train loss 0.4908499122\n",
      "After 170 epochs, train loss 0.4898899496\n",
      "After 180 epochs, train loss 0.4890968204\n",
      "After 190 epochs, train loss 0.4884407818\n",
      "After 200 epochs, train loss 0.4878978133\n",
      "After 210 epochs, train loss 0.4874480367\n",
      "After 220 epochs, train loss 0.4870754480\n",
      "After 230 epochs, train loss 0.4867665172\n",
      "After 240 epochs, train loss 0.4865105152\n",
      "After 250 epochs, train loss 0.4862982929\n",
      "After 260 epochs, train loss 0.4861223996\n",
      "After 270 epochs, train loss 0.4859766960\n",
      "After 280 epochs, train loss 0.4858559072\n",
      "After 290 epochs, train loss 0.4857558310\n",
      "After 300 epochs, train loss 0.4856729507\n",
      "After 310 epochs, train loss 0.4856043458\n",
      "After 320 epochs, train loss 0.4855475724\n",
      "After 330 epochs, train loss 0.4855005443\n",
      "After 340 epochs, train loss 0.4854616225\n",
      "After 350 epochs, train loss 0.4854294956\n",
      "After 360 epochs, train loss 0.4854028821\n",
      "After 370 epochs, train loss 0.4853808284\n",
      "After 380 epochs, train loss 0.4853626788\n",
      "After 390 epochs, train loss 0.4853476584\n",
      "After 400 epochs, train loss 0.4853352308\n",
      "After 410 epochs, train loss 0.4853249788\n",
      "After 420 epochs, train loss 0.4853165448\n",
      "After 430 epochs, train loss 0.4853095412\n",
      "After 440 epochs, train loss 0.4853037894\n",
      "After 450 epochs, train loss 0.4852989912\n",
      "After 460 epochs, train loss 0.4852950871\n",
      "After 470 epochs, train loss 0.4852918386\n",
      "After 480 epochs, train loss 0.4852891862\n",
      "After 490 epochs, train loss 0.4852869213\n",
      "Final RMSE: 0.6966\n",
      "Recall: tensor(0.0002)\n",
      "Pass 3\n",
      "After 0 epochs, train loss 2.9567718506\n",
      "After 10 epochs, train loss 0.9444071651\n",
      "After 20 epochs, train loss 0.6938497424\n",
      "After 30 epochs, train loss 0.6036570072\n",
      "After 40 epochs, train loss 0.5628322363\n",
      "After 50 epochs, train loss 0.5410323143\n",
      "After 60 epochs, train loss 0.5278107524\n",
      "After 70 epochs, train loss 0.5188154578\n",
      "After 80 epochs, train loss 0.5122422576\n",
      "After 90 epochs, train loss 0.5072112679\n",
      "After 100 epochs, train loss 0.5032554865\n",
      "After 110 epochs, train loss 0.5000931025\n",
      "After 120 epochs, train loss 0.4975377023\n",
      "After 130 epochs, train loss 0.4954577684\n",
      "After 140 epochs, train loss 0.4937562644\n",
      "After 150 epochs, train loss 0.4923590124\n",
      "After 160 epochs, train loss 0.4912086427\n",
      "After 170 epochs, train loss 0.4902594686\n",
      "After 180 epochs, train loss 0.4894751310\n",
      "After 190 epochs, train loss 0.4888263047\n",
      "After 200 epochs, train loss 0.4882890284\n",
      "After 210 epochs, train loss 0.4878440797\n",
      "After 220 epochs, train loss 0.4874752760\n",
      "After 230 epochs, train loss 0.4871695936\n",
      "After 240 epochs, train loss 0.4869162142\n",
      "After 250 epochs, train loss 0.4867061377\n",
      "After 260 epochs, train loss 0.4865320325\n",
      "After 270 epochs, train loss 0.4863876998\n",
      "After 280 epochs, train loss 0.4862680733\n",
      "After 290 epochs, train loss 0.4861690104\n",
      "After 300 epochs, train loss 0.4860869348\n",
      "After 310 epochs, train loss 0.4860189855\n",
      "After 320 epochs, train loss 0.4859627187\n",
      "After 330 epochs, train loss 0.4859161675\n",
      "After 340 epochs, train loss 0.4858776033\n",
      "After 350 epochs, train loss 0.4858458042\n",
      "After 360 epochs, train loss 0.4858194292\n",
      "After 370 epochs, train loss 0.4857976139\n",
      "After 380 epochs, train loss 0.4857796431\n",
      "After 390 epochs, train loss 0.4857646823\n",
      "After 400 epochs, train loss 0.4857524335\n",
      "After 410 epochs, train loss 0.4857423007\n",
      "After 420 epochs, train loss 0.4857338965\n",
      "After 430 epochs, train loss 0.4857269824\n",
      "After 440 epochs, train loss 0.4857212901\n",
      "After 450 epochs, train loss 0.4857165515\n",
      "After 460 epochs, train loss 0.4857126772\n",
      "After 470 epochs, train loss 0.4857094586\n",
      "After 480 epochs, train loss 0.4857067764\n",
      "After 490 epochs, train loss 0.4857046008\n",
      "Final RMSE: 0.6969\n",
      "Recall: tensor(6.0606e-05)\n",
      "Pass 4\n",
      "After 0 epochs, train loss 3.0324254036\n",
      "After 10 epochs, train loss 0.9588482380\n",
      "After 20 epochs, train loss 0.7004000545\n",
      "After 30 epochs, train loss 0.6055476665\n",
      "After 40 epochs, train loss 0.5629808903\n",
      "After 50 epochs, train loss 0.5407494903\n",
      "After 60 epochs, train loss 0.5274611712\n",
      "After 70 epochs, train loss 0.5185052752\n",
      "After 80 epochs, train loss 0.5119788647\n",
      "After 90 epochs, train loss 0.5069943070\n",
      "After 100 epochs, train loss 0.5030782223\n",
      "After 110 epochs, train loss 0.4999487698\n",
      "After 120 epochs, train loss 0.4974203408\n",
      "After 130 epochs, train loss 0.4953624010\n",
      "After 140 epochs, train loss 0.4936787486\n",
      "After 150 epochs, train loss 0.4922961891\n",
      "After 160 epochs, train loss 0.4911577404\n",
      "After 170 epochs, train loss 0.4902184308\n",
      "After 180 epochs, train loss 0.4894421995\n",
      "After 190 epochs, train loss 0.4888000488\n",
      "After 200 epochs, train loss 0.4882682860\n",
      "After 210 epochs, train loss 0.4878278375\n",
      "After 220 epochs, train loss 0.4874628484\n",
      "After 230 epochs, train loss 0.4871602952\n",
      "After 240 epochs, train loss 0.4869094491\n",
      "After 250 epochs, train loss 0.4867014885\n",
      "After 260 epochs, train loss 0.4865291715\n",
      "After 270 epochs, train loss 0.4863863289\n",
      "After 280 epochs, train loss 0.4862679541\n",
      "After 290 epochs, train loss 0.4861698747\n",
      "After 300 epochs, train loss 0.4860886335\n",
      "After 310 epochs, train loss 0.4860213995\n",
      "After 320 epochs, train loss 0.4859657288\n",
      "After 330 epochs, train loss 0.4859196246\n",
      "After 340 epochs, train loss 0.4858814180\n",
      "After 350 epochs, train loss 0.4858498871\n",
      "After 360 epochs, train loss 0.4858238101\n",
      "After 370 epochs, train loss 0.4858022332\n",
      "After 380 epochs, train loss 0.4857844114\n",
      "After 390 epochs, train loss 0.4857696891\n",
      "After 400 epochs, train loss 0.4857574999\n",
      "After 410 epochs, train loss 0.4857474864\n",
      "After 420 epochs, train loss 0.4857391715\n",
      "After 430 epochs, train loss 0.4857322872\n",
      "After 440 epochs, train loss 0.4857267141\n",
      "After 450 epochs, train loss 0.4857220352\n",
      "After 460 epochs, train loss 0.4857181609\n",
      "After 470 epochs, train loss 0.4857150018\n",
      "After 480 epochs, train loss 0.4857124090\n",
      "After 490 epochs, train loss 0.4857102036\n",
      "Final RMSE: 0.6969\n",
      "Recall: tensor(0.0002)\n",
      "Pass 0\n",
      "After 0 epochs, train loss 15.3206262589\n",
      "After 10 epochs, train loss 3.2793262005\n",
      "After 20 epochs, train loss 1.2715489864\n",
      "After 30 epochs, train loss 1.0024377108\n",
      "After 40 epochs, train loss 0.8539772630\n",
      "After 50 epochs, train loss 0.7506299615\n",
      "After 60 epochs, train loss 0.6964457035\n",
      "After 70 epochs, train loss 0.6688410044\n",
      "After 80 epochs, train loss 0.6524679661\n",
      "After 90 epochs, train loss 0.6413101554\n",
      "After 100 epochs, train loss 0.6331382990\n",
      "After 110 epochs, train loss 0.6269229650\n",
      "After 120 epochs, train loss 0.6220796704\n",
      "After 130 epochs, train loss 0.6182405949\n",
      "After 140 epochs, train loss 0.6151602268\n",
      "After 150 epochs, train loss 0.6126664877\n",
      "After 160 epochs, train loss 0.6106345057\n",
      "After 170 epochs, train loss 0.6089708209\n",
      "After 180 epochs, train loss 0.6076036096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 190 epochs, train loss 0.6064770818\n",
      "After 200 epochs, train loss 0.6055467725\n",
      "After 210 epochs, train loss 0.6047773957\n",
      "After 220 epochs, train loss 0.6041402817\n",
      "After 230 epochs, train loss 0.6036123633\n",
      "After 240 epochs, train loss 0.6031745672\n",
      "After 250 epochs, train loss 0.6028113961\n",
      "After 260 epochs, train loss 0.6025100350\n",
      "After 270 epochs, train loss 0.6022599936\n",
      "After 280 epochs, train loss 0.6020524502\n",
      "After 290 epochs, train loss 0.6018802524\n",
      "After 300 epochs, train loss 0.6017373204\n",
      "After 310 epochs, train loss 0.6016187668\n",
      "After 320 epochs, train loss 0.6015203595\n",
      "After 330 epochs, train loss 0.6014388204\n",
      "After 340 epochs, train loss 0.6013712287\n",
      "After 350 epochs, train loss 0.6013151407\n",
      "After 360 epochs, train loss 0.6012686491\n",
      "After 370 epochs, train loss 0.6012302041\n",
      "After 380 epochs, train loss 0.6011983156\n",
      "After 390 epochs, train loss 0.6011718512\n",
      "After 400 epochs, train loss 0.6011499763\n",
      "After 410 epochs, train loss 0.6011319160\n",
      "After 420 epochs, train loss 0.6011168957\n",
      "After 430 epochs, train loss 0.6011044979\n",
      "After 440 epochs, train loss 0.6010943055\n",
      "After 450 epochs, train loss 0.6010857821\n",
      "After 460 epochs, train loss 0.6010787487\n",
      "After 470 epochs, train loss 0.6010730267\n",
      "After 480 epochs, train loss 0.6010681987\n",
      "After 490 epochs, train loss 0.6010642648\n",
      "Final RMSE: 0.7753\n",
      "Recall: tensor(0.0001)\n",
      "Pass 1\n",
      "After 0 epochs, train loss 15.2828426361\n",
      "After 10 epochs, train loss 3.2874491215\n",
      "After 20 epochs, train loss 1.2905346155\n",
      "After 30 epochs, train loss 1.0157110691\n",
      "After 40 epochs, train loss 0.8612311482\n",
      "After 50 epochs, train loss 0.7551676035\n",
      "After 60 epochs, train loss 0.7000551224\n",
      "After 70 epochs, train loss 0.6720077395\n",
      "After 80 epochs, train loss 0.6552966237\n",
      "After 90 epochs, train loss 0.6438667774\n",
      "After 100 epochs, train loss 0.6354866624\n",
      "After 110 epochs, train loss 0.6291145682\n",
      "After 120 epochs, train loss 0.6241525412\n",
      "After 130 epochs, train loss 0.6202223897\n",
      "After 140 epochs, train loss 0.6170709729\n",
      "After 150 epochs, train loss 0.6145212650\n",
      "After 160 epochs, train loss 0.6124446988\n",
      "After 170 epochs, train loss 0.6107451916\n",
      "After 180 epochs, train loss 0.6093491316\n",
      "After 190 epochs, train loss 0.6081989408\n",
      "After 200 epochs, train loss 0.6072493792\n",
      "After 210 epochs, train loss 0.6064642668\n",
      "After 220 epochs, train loss 0.6058143377\n",
      "After 230 epochs, train loss 0.6052757502\n",
      "After 240 epochs, train loss 0.6048292518\n",
      "After 250 epochs, train loss 0.6044588089\n",
      "After 260 epochs, train loss 0.6041514874\n",
      "After 270 epochs, train loss 0.6038964987\n",
      "After 280 epochs, train loss 0.6036849022\n",
      "After 290 epochs, train loss 0.6035092473\n",
      "After 300 epochs, train loss 0.6033635736\n",
      "After 310 epochs, train loss 0.6032426357\n",
      "After 320 epochs, train loss 0.6031423807\n",
      "After 330 epochs, train loss 0.6030592322\n",
      "After 340 epochs, train loss 0.6029902697\n",
      "After 350 epochs, train loss 0.6029331088\n",
      "After 360 epochs, train loss 0.6028857231\n",
      "After 370 epochs, train loss 0.6028465033\n",
      "After 380 epochs, train loss 0.6028139591\n",
      "After 390 epochs, train loss 0.6027870178\n",
      "After 400 epochs, train loss 0.6027647853\n",
      "After 410 epochs, train loss 0.6027463078\n",
      "After 420 epochs, train loss 0.6027310491\n",
      "After 430 epochs, train loss 0.6027183533\n",
      "After 440 epochs, train loss 0.6027079225\n",
      "After 450 epochs, train loss 0.6026993394\n",
      "After 460 epochs, train loss 0.6026921868\n",
      "After 470 epochs, train loss 0.6026863456\n",
      "After 480 epochs, train loss 0.6026813984\n",
      "After 490 epochs, train loss 0.6026773453\n",
      "Final RMSE: 0.7763\n",
      "Recall: tensor(0.0002)\n",
      "Pass 2\n",
      "After 0 epochs, train loss 15.6070556641\n",
      "After 10 epochs, train loss 3.3724882603\n",
      "After 20 epochs, train loss 1.2963788509\n",
      "After 30 epochs, train loss 1.0149800777\n",
      "After 40 epochs, train loss 0.8652252555\n",
      "After 50 epochs, train loss 0.7600660920\n",
      "After 60 epochs, train loss 0.7039203644\n",
      "After 70 epochs, train loss 0.6750126481\n",
      "After 80 epochs, train loss 0.6578763127\n",
      "After 90 epochs, train loss 0.6462387443\n",
      "After 100 epochs, train loss 0.6377338171\n",
      "After 110 epochs, train loss 0.6312731504\n",
      "After 120 epochs, train loss 0.6262437701\n",
      "After 130 epochs, train loss 0.6222610474\n",
      "After 140 epochs, train loss 0.6190681458\n",
      "After 150 epochs, train loss 0.6164854765\n",
      "After 160 epochs, train loss 0.6143825650\n",
      "After 170 epochs, train loss 0.6126617789\n",
      "After 180 epochs, train loss 0.6112483740\n",
      "After 190 epochs, train loss 0.6100842953\n",
      "After 200 epochs, train loss 0.6091232300\n",
      "After 210 epochs, train loss 0.6083287597\n",
      "After 220 epochs, train loss 0.6076710224\n",
      "After 230 epochs, train loss 0.6071261764\n",
      "After 240 epochs, train loss 0.6066743731\n",
      "After 250 epochs, train loss 0.6062996387\n",
      "After 260 epochs, train loss 0.6059888005\n",
      "After 270 epochs, train loss 0.6057308316\n",
      "After 280 epochs, train loss 0.6055167317\n",
      "After 290 epochs, train loss 0.6053391099\n",
      "After 300 epochs, train loss 0.6051917672\n",
      "After 310 epochs, train loss 0.6050694585\n",
      "After 320 epochs, train loss 0.6049680114\n",
      "After 330 epochs, train loss 0.6048839688\n",
      "After 340 epochs, train loss 0.6048141718\n",
      "After 350 epochs, train loss 0.6047564149\n",
      "After 360 epochs, train loss 0.6047084928\n",
      "After 370 epochs, train loss 0.6046687961\n",
      "After 380 epochs, train loss 0.6046358943\n",
      "After 390 epochs, train loss 0.6046087146\n",
      "After 400 epochs, train loss 0.6045861840\n",
      "After 410 epochs, train loss 0.6045674682\n",
      "After 420 epochs, train loss 0.6045520306\n",
      "After 430 epochs, train loss 0.6045392752\n",
      "After 440 epochs, train loss 0.6045287251\n",
      "After 450 epochs, train loss 0.6045200229\n",
      "After 460 epochs, train loss 0.6045127511\n",
      "After 470 epochs, train loss 0.6045067906\n",
      "After 480 epochs, train loss 0.6045018435\n",
      "After 490 epochs, train loss 0.6044977903\n",
      "Final RMSE: 0.7775\n",
      "Recall: tensor(0.0003)\n",
      "Pass 3\n",
      "After 0 epochs, train loss 15.6352472305\n",
      "After 10 epochs, train loss 3.3967785835\n",
      "After 20 epochs, train loss 1.3136492968\n",
      "After 30 epochs, train loss 1.0222543478\n",
      "After 40 epochs, train loss 0.8669092059\n",
      "After 50 epochs, train loss 0.7601087689\n",
      "After 60 epochs, train loss 0.7037509084\n",
      "After 70 epochs, train loss 0.6748378277\n",
      "After 80 epochs, train loss 0.6576713324\n",
      "After 90 epochs, train loss 0.6459979415\n",
      "After 100 epochs, train loss 0.6374692917\n",
      "After 110 epochs, train loss 0.6309964657\n",
      "After 120 epochs, train loss 0.6259622574\n",
      "After 130 epochs, train loss 0.6219785810\n",
      "After 140 epochs, train loss 0.6187865138\n",
      "After 150 epochs, train loss 0.6162055731\n",
      "After 160 epochs, train loss 0.6141045690\n",
      "After 170 epochs, train loss 0.6123856902\n",
      "After 180 epochs, train loss 0.6109741330\n",
      "After 190 epochs, train loss 0.6098115444\n",
      "After 200 epochs, train loss 0.6088519692\n",
      "After 210 epochs, train loss 0.6080586910\n",
      "After 220 epochs, train loss 0.6074019670\n",
      "After 230 epochs, train loss 0.6068579555\n",
      "After 240 epochs, train loss 0.6064068675\n",
      "After 250 epochs, train loss 0.6060327888\n",
      "After 260 epochs, train loss 0.6057223678\n",
      "After 270 epochs, train loss 0.6054648161\n",
      "After 280 epochs, train loss 0.6052510738\n",
      "After 290 epochs, train loss 0.6050736904\n",
      "After 300 epochs, train loss 0.6049265265\n",
      "After 310 epochs, train loss 0.6048044562\n",
      "After 320 epochs, train loss 0.6047031879\n",
      "After 330 epochs, train loss 0.6046192050\n",
      "After 340 epochs, train loss 0.6045495272\n",
      "After 350 epochs, train loss 0.6044917703\n",
      "After 360 epochs, train loss 0.6044439673\n",
      "After 370 epochs, train loss 0.6044043303\n",
      "After 380 epochs, train loss 0.6043714881\n",
      "After 390 epochs, train loss 0.6043443084\n",
      "After 400 epochs, train loss 0.6043217778\n",
      "After 410 epochs, train loss 0.6043031812\n",
      "After 420 epochs, train loss 0.6042877436\n",
      "After 430 epochs, train loss 0.6042749882\n",
      "After 440 epochs, train loss 0.6042644382\n",
      "After 450 epochs, train loss 0.6042556763\n",
      "After 460 epochs, train loss 0.6042484641\n",
      "After 470 epochs, train loss 0.6042425036\n",
      "After 480 epochs, train loss 0.6042376161\n",
      "After 490 epochs, train loss 0.6042335033\n",
      "Final RMSE: 0.7773\n",
      "Recall: tensor(6.0606e-05)\n",
      "Pass 4\n",
      "After 0 epochs, train loss 15.5002651215\n",
      "After 10 epochs, train loss 3.3661730289\n",
      "After 20 epochs, train loss 1.3110610247\n",
      "After 30 epochs, train loss 1.0219905376\n",
      "After 40 epochs, train loss 0.8660342693\n",
      "After 50 epochs, train loss 0.7594701648\n",
      "After 60 epochs, train loss 0.7034054399\n",
      "After 70 epochs, train loss 0.6745642424\n",
      "After 80 epochs, train loss 0.6573703289\n",
      "After 90 epochs, train loss 0.6456509829\n",
      "After 100 epochs, train loss 0.6370783448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 110 epochs, train loss 0.6305670738\n",
      "After 120 epochs, train loss 0.6254999638\n",
      "After 130 epochs, train loss 0.6214887500\n",
      "After 140 epochs, train loss 0.6182740331\n",
      "After 150 epochs, train loss 0.6156743169\n",
      "After 160 epochs, train loss 0.6135578752\n",
      "After 170 epochs, train loss 0.6118263006\n",
      "After 180 epochs, train loss 0.6104041934\n",
      "After 190 epochs, train loss 0.6092330813\n",
      "After 200 epochs, train loss 0.6082664132\n",
      "After 210 epochs, train loss 0.6074671745\n",
      "After 220 epochs, train loss 0.6068056226\n",
      "After 230 epochs, train loss 0.6062576175\n",
      "After 240 epochs, train loss 0.6058031917\n",
      "After 250 epochs, train loss 0.6054263711\n",
      "After 260 epochs, train loss 0.6051136851\n",
      "After 270 epochs, train loss 0.6048542261\n",
      "After 280 epochs, train loss 0.6046389341\n",
      "After 290 epochs, train loss 0.6044602394\n",
      "After 300 epochs, train loss 0.6043120623\n",
      "After 310 epochs, train loss 0.6041890979\n",
      "After 320 epochs, train loss 0.6040870547\n",
      "After 330 epochs, train loss 0.6040024757\n",
      "After 340 epochs, train loss 0.6039323211\n",
      "After 350 epochs, train loss 0.6038741469\n",
      "After 360 epochs, train loss 0.6038259864\n",
      "After 370 epochs, train loss 0.6037861109\n",
      "After 380 epochs, train loss 0.6037530303\n",
      "After 390 epochs, train loss 0.6037256122\n",
      "After 400 epochs, train loss 0.6037029624\n",
      "After 410 epochs, train loss 0.6036841869\n",
      "After 420 epochs, train loss 0.6036686301\n",
      "After 430 epochs, train loss 0.6036558151\n",
      "After 440 epochs, train loss 0.6036452055\n",
      "After 450 epochs, train loss 0.6036363244\n",
      "After 460 epochs, train loss 0.6036290526\n",
      "After 470 epochs, train loss 0.6036230922\n",
      "After 480 epochs, train loss 0.6036181450\n",
      "After 490 epochs, train loss 0.6036140323\n",
      "Final RMSE: 0.7769\n",
      "Recall: tensor(0.0003)\n",
      "Pass 0\n",
      "After 0 epochs, train loss 77.7004547119\n",
      "After 10 epochs, train loss 23.6656761169\n",
      "After 20 epochs, train loss 7.4448385239\n",
      "After 30 epochs, train loss 3.0927040577\n",
      "After 40 epochs, train loss 1.8707540035\n",
      "After 50 epochs, train loss 1.4556454420\n",
      "After 60 epochs, train loss 1.2718454599\n",
      "After 70 epochs, train loss 1.1695880890\n",
      "After 80 epochs, train loss 1.1037824154\n",
      "After 90 epochs, train loss 1.0577601194\n",
      "After 100 epochs, train loss 1.0239648819\n",
      "After 110 epochs, train loss 0.9983700514\n",
      "After 120 epochs, train loss 0.9785714149\n",
      "After 130 epochs, train loss 0.9630181193\n",
      "After 140 epochs, train loss 0.9506556988\n",
      "After 150 epochs, train loss 0.9407392144\n",
      "After 160 epochs, train loss 0.9327266812\n",
      "After 170 epochs, train loss 0.9262149930\n",
      "After 180 epochs, train loss 0.9208984971\n",
      "After 190 epochs, train loss 0.9165414572\n",
      "After 200 epochs, train loss 0.9129602909\n",
      "After 210 epochs, train loss 0.9100098610\n",
      "After 220 epochs, train loss 0.9075744748\n",
      "After 230 epochs, train loss 0.9055613279\n",
      "After 240 epochs, train loss 0.9038953185\n",
      "After 250 epochs, train loss 0.9025152922\n",
      "After 260 epochs, train loss 0.9013717175\n",
      "After 270 epochs, train loss 0.9004232883\n",
      "After 280 epochs, train loss 0.8996367455\n",
      "After 290 epochs, train loss 0.8989843130\n",
      "After 300 epochs, train loss 0.8984429240\n",
      "After 310 epochs, train loss 0.8979937434\n",
      "After 320 epochs, train loss 0.8976209164\n",
      "After 330 epochs, train loss 0.8973118663\n",
      "After 340 epochs, train loss 0.8970555067\n",
      "After 350 epochs, train loss 0.8968428373\n",
      "After 360 epochs, train loss 0.8966664672\n",
      "After 370 epochs, train loss 0.8965204358\n",
      "After 380 epochs, train loss 0.8963991404\n",
      "After 390 epochs, train loss 0.8962987661\n",
      "After 400 epochs, train loss 0.8962155581\n",
      "After 410 epochs, train loss 0.8961466551\n",
      "After 420 epochs, train loss 0.8960896134\n",
      "After 430 epochs, train loss 0.8960424066\n",
      "After 440 epochs, train loss 0.8960033059\n",
      "After 450 epochs, train loss 0.8959710598\n",
      "After 460 epochs, train loss 0.8959441781\n",
      "After 470 epochs, train loss 0.8959220648\n",
      "After 480 epochs, train loss 0.8959038258\n",
      "After 490 epochs, train loss 0.8958886266\n",
      "Final RMSE: 0.9465\n",
      "Recall: tensor(0.0004)\n",
      "Pass 1\n",
      "After 0 epochs, train loss 78.2476654053\n",
      "After 10 epochs, train loss 23.9204730988\n",
      "After 20 epochs, train loss 7.5626077652\n",
      "After 30 epochs, train loss 3.1522257328\n",
      "After 40 epochs, train loss 1.9058741331\n",
      "After 50 epochs, train loss 1.4798288345\n",
      "After 60 epochs, train loss 1.2905135155\n",
      "After 70 epochs, train loss 1.1850963831\n",
      "After 80 epochs, train loss 1.1172789335\n",
      "After 90 epochs, train loss 1.0698764324\n",
      "After 100 epochs, train loss 1.0350859165\n",
      "After 110 epochs, train loss 1.0087485313\n",
      "After 120 epochs, train loss 0.9883825779\n",
      "After 130 epochs, train loss 0.9723879695\n",
      "After 140 epochs, train loss 0.9596778154\n",
      "After 150 epochs, train loss 0.9494841099\n",
      "After 160 epochs, train loss 0.9412490129\n",
      "After 170 epochs, train loss 0.9345573187\n",
      "After 180 epochs, train loss 0.9290942550\n",
      "After 190 epochs, train loss 0.9246177673\n",
      "After 200 epochs, train loss 0.9209385514\n",
      "After 210 epochs, train loss 0.9179075360\n",
      "After 220 epochs, train loss 0.9154057503\n",
      "After 230 epochs, train loss 0.9133378863\n",
      "After 240 epochs, train loss 0.9116263986\n",
      "After 250 epochs, train loss 0.9102091193\n",
      "After 260 epochs, train loss 0.9090344906\n",
      "After 270 epochs, train loss 0.9080605507\n",
      "After 280 epochs, train loss 0.9072524905\n",
      "After 290 epochs, train loss 0.9065824151\n",
      "After 300 epochs, train loss 0.9060264230\n",
      "After 310 epochs, train loss 0.9055651426\n",
      "After 320 epochs, train loss 0.9051824212\n",
      "After 330 epochs, train loss 0.9048648477\n",
      "After 340 epochs, train loss 0.9046014547\n",
      "After 350 epochs, train loss 0.9043831229\n",
      "After 360 epochs, train loss 0.9042021036\n",
      "After 370 epochs, train loss 0.9040519595\n",
      "After 380 epochs, train loss 0.9039275050\n",
      "After 390 epochs, train loss 0.9038244486\n",
      "After 400 epochs, train loss 0.9037389159\n",
      "After 410 epochs, train loss 0.9036682248\n",
      "After 420 epochs, train loss 0.9036098123\n",
      "After 430 epochs, train loss 0.9035612345\n",
      "After 440 epochs, train loss 0.9035211205\n",
      "After 450 epochs, train loss 0.9034878016\n",
      "After 460 epochs, train loss 0.9034603834\n",
      "After 470 epochs, train loss 0.9034376740\n",
      "After 480 epochs, train loss 0.9034188390\n",
      "After 490 epochs, train loss 0.9034033418\n",
      "Final RMSE: 0.9505\n",
      "Recall: tensor(0.0003)\n",
      "Pass 2\n",
      "After 0 epochs, train loss 77.3571243286\n",
      "After 10 epochs, train loss 23.5256633759\n",
      "After 20 epochs, train loss 7.3952336311\n",
      "After 30 epochs, train loss 3.0774476528\n",
      "After 40 epochs, train loss 1.8673596382\n",
      "After 50 epochs, train loss 1.4561265707\n",
      "After 60 epochs, train loss 1.2734737396\n",
      "After 70 epochs, train loss 1.1714489460\n",
      "After 80 epochs, train loss 1.1055836678\n",
      "After 90 epochs, train loss 1.0594226122\n",
      "After 100 epochs, train loss 1.0254808664\n",
      "After 110 epochs, train loss 0.9997537136\n",
      "After 120 epochs, train loss 0.9798420668\n",
      "After 130 epochs, train loss 0.9641941786\n",
      "After 140 epochs, train loss 0.9517537355\n",
      "After 150 epochs, train loss 0.9417726398\n",
      "After 160 epochs, train loss 0.9337069988\n",
      "After 170 epochs, train loss 0.9271515608\n",
      "After 180 epochs, train loss 0.9217990041\n",
      "After 190 epochs, train loss 0.9174121618\n",
      "After 200 epochs, train loss 0.9138063788\n",
      "After 210 epochs, train loss 0.9108355641\n",
      "After 220 epochs, train loss 0.9083831906\n",
      "After 230 epochs, train loss 0.9063559771\n",
      "After 240 epochs, train loss 0.9046784043\n",
      "After 250 epochs, train loss 0.9032889009\n",
      "After 260 epochs, train loss 0.9021372199\n",
      "After 270 epochs, train loss 0.9011823535\n",
      "After 280 epochs, train loss 0.9003902674\n",
      "After 290 epochs, train loss 0.8997331262\n",
      "After 300 epochs, train loss 0.8991879225\n",
      "After 310 epochs, train loss 0.8987357020\n",
      "After 320 epochs, train loss 0.8983603120\n",
      "After 330 epochs, train loss 0.8980491757\n",
      "After 340 epochs, train loss 0.8977909088\n",
      "After 350 epochs, train loss 0.8975766301\n",
      "After 360 epochs, train loss 0.8973992467\n",
      "After 370 epochs, train loss 0.8972520232\n",
      "After 380 epochs, train loss 0.8971300721\n",
      "After 390 epochs, train loss 0.8970289826\n",
      "After 400 epochs, train loss 0.8969451785\n",
      "After 410 epochs, train loss 0.8968757987\n",
      "After 420 epochs, train loss 0.8968183398\n",
      "After 430 epochs, train loss 0.8967708945\n",
      "After 440 epochs, train loss 0.8967314959\n",
      "After 450 epochs, train loss 0.8966989517\n",
      "After 460 epochs, train loss 0.8966719508\n",
      "After 470 epochs, train loss 0.8966496587\n",
      "After 480 epochs, train loss 0.8966311812\n",
      "After 490 epochs, train loss 0.8966160417\n",
      "Final RMSE: 0.9469\n",
      "Recall: tensor(0.0004)\n",
      "Pass 3\n",
      "After 0 epochs, train loss 78.4148941040\n",
      "After 10 epochs, train loss 23.9884204865\n",
      "After 20 epochs, train loss 7.5853824615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 30 epochs, train loss 3.1568825245\n",
      "After 40 epochs, train loss 1.9038653374\n",
      "After 50 epochs, train loss 1.4755847454\n",
      "After 60 epochs, train loss 1.2856581211\n",
      "After 70 epochs, train loss 1.1801937819\n",
      "After 80 epochs, train loss 1.1125078201\n",
      "After 90 epochs, train loss 1.0652793646\n",
      "After 100 epochs, train loss 1.0306591988\n",
      "After 110 epochs, train loss 1.0044738054\n",
      "After 120 epochs, train loss 0.9842387438\n",
      "After 130 epochs, train loss 0.9683552384\n",
      "After 140 epochs, train loss 0.9557381272\n",
      "After 150 epochs, train loss 0.9456222653\n",
      "After 160 epochs, train loss 0.9374520779\n",
      "After 170 epochs, train loss 0.9308143854\n",
      "After 180 epochs, train loss 0.9253964424\n",
      "After 190 epochs, train loss 0.9209574461\n",
      "After 200 epochs, train loss 0.9173094034\n",
      "After 210 epochs, train loss 0.9143042564\n",
      "After 220 epochs, train loss 0.9118241072\n",
      "After 230 epochs, train loss 0.9097741246\n",
      "After 240 epochs, train loss 0.9080778956\n",
      "After 250 epochs, train loss 0.9066729546\n",
      "After 260 epochs, train loss 0.9055085182\n",
      "After 270 epochs, train loss 0.9045431018\n",
      "After 280 epochs, train loss 0.9037423134\n",
      "After 290 epochs, train loss 0.9030781388\n",
      "After 300 epochs, train loss 0.9025270939\n",
      "After 310 epochs, train loss 0.9020698071\n",
      "After 320 epochs, train loss 0.9016905427\n",
      "After 330 epochs, train loss 0.9013758898\n",
      "After 340 epochs, train loss 0.9011148214\n",
      "After 350 epochs, train loss 0.9008983970\n",
      "After 360 epochs, train loss 0.9007189274\n",
      "After 370 epochs, train loss 0.9005702138\n",
      "After 380 epochs, train loss 0.9004468918\n",
      "After 390 epochs, train loss 0.9003446102\n",
      "After 400 epochs, train loss 0.9002600312\n",
      "After 410 epochs, train loss 0.9001898766\n",
      "After 420 epochs, train loss 0.9001318812\n",
      "After 430 epochs, train loss 0.9000837803\n",
      "After 440 epochs, train loss 0.9000440240\n",
      "After 450 epochs, train loss 0.9000110030\n",
      "After 460 epochs, train loss 0.8999837637\n",
      "After 470 epochs, train loss 0.8999612331\n",
      "After 480 epochs, train loss 0.8999426365\n",
      "After 490 epochs, train loss 0.8999271393\n",
      "Final RMSE: 0.9486\n",
      "Recall: tensor(0.0003)\n",
      "Pass 4\n",
      "After 0 epochs, train loss 77.8624725342\n",
      "After 10 epochs, train loss 23.7804813385\n",
      "After 20 epochs, train loss 7.5273523331\n",
      "After 30 epochs, train loss 3.1523749828\n",
      "After 40 epochs, train loss 1.9148529768\n",
      "After 50 epochs, train loss 1.4894953966\n",
      "After 60 epochs, train loss 1.2989560366\n",
      "After 70 epochs, train loss 1.1921676397\n",
      "After 80 epochs, train loss 1.1232204437\n",
      "After 90 epochs, train loss 1.0749522448\n",
      "After 100 epochs, train loss 1.0395071507\n",
      "After 110 epochs, train loss 1.0126726627\n",
      "After 120 epochs, train loss 0.9919248223\n",
      "After 130 epochs, train loss 0.9756337404\n",
      "After 140 epochs, train loss 0.9626906514\n",
      "After 150 epochs, train loss 0.9523124099\n",
      "After 160 epochs, train loss 0.9439297318\n",
      "After 170 epochs, train loss 0.9371191859\n",
      "After 180 epochs, train loss 0.9315600395\n",
      "After 190 epochs, train loss 0.9270053506\n",
      "After 200 epochs, train loss 0.9232623577\n",
      "After 210 epochs, train loss 0.9201789498\n",
      "After 220 epochs, train loss 0.9176341891\n",
      "After 230 epochs, train loss 0.9155309200\n",
      "After 240 epochs, train loss 0.9137904644\n",
      "After 250 epochs, train loss 0.9123490453\n",
      "After 260 epochs, train loss 0.9111543894\n",
      "After 270 epochs, train loss 0.9101639390\n",
      "After 280 epochs, train loss 0.9093425274\n",
      "After 290 epochs, train loss 0.9086610675\n",
      "After 300 epochs, train loss 0.9080955982\n",
      "After 310 epochs, train loss 0.9076266289\n",
      "After 320 epochs, train loss 0.9072374701\n",
      "After 330 epochs, train loss 0.9069146514\n",
      "After 340 epochs, train loss 0.9066469073\n",
      "After 350 epochs, train loss 0.9064249396\n",
      "After 360 epochs, train loss 0.9062408209\n",
      "After 370 epochs, train loss 0.9060881734\n",
      "After 380 epochs, train loss 0.9059617519\n",
      "After 390 epochs, train loss 0.9058569074\n",
      "After 400 epochs, train loss 0.9057701230\n",
      "After 410 epochs, train loss 0.9056981802\n",
      "After 420 epochs, train loss 0.9056385756\n",
      "After 430 epochs, train loss 0.9055892825\n",
      "After 440 epochs, train loss 0.9055485129\n",
      "After 450 epochs, train loss 0.9055147171\n",
      "After 460 epochs, train loss 0.9054867625\n",
      "After 470 epochs, train loss 0.9054636359\n",
      "After 480 epochs, train loss 0.9054445624\n",
      "After 490 epochs, train loss 0.9054288268\n",
      "Final RMSE: 0.9515\n",
      "Recall: tensor(0.0007)\n",
      "Pass 0\n",
      "After 0 epochs, train loss 225.3349914551\n",
      "After 10 epochs, train loss 80.8071670532\n",
      "After 20 epochs, train loss 30.6170730591\n",
      "After 30 epochs, train loss 13.7997674942\n",
      "After 40 epochs, train loss 7.7099828720\n",
      "After 50 epochs, train loss 5.1772508621\n",
      "After 60 epochs, train loss 3.9510302544\n",
      "After 70 epochs, train loss 3.2711892128\n",
      "After 80 epochs, train loss 2.8520429134\n",
      "After 90 epochs, train loss 2.5729591846\n",
      "After 100 epochs, train loss 2.3768162727\n",
      "After 110 epochs, train loss 2.2335932255\n",
      "After 120 epochs, train loss 2.1260643005\n",
      "After 130 epochs, train loss 2.0436303616\n",
      "After 140 epochs, train loss 1.9794080257\n",
      "After 150 epochs, train loss 1.9287352562\n",
      "After 160 epochs, train loss 1.8883465528\n",
      "After 170 epochs, train loss 1.8558914661\n",
      "After 180 epochs, train loss 1.8296402693\n",
      "After 190 epochs, train loss 1.8082938194\n",
      "After 200 epochs, train loss 1.7908610106\n",
      "After 210 epochs, train loss 1.7765754461\n",
      "After 220 epochs, train loss 1.7648359537\n",
      "After 230 epochs, train loss 1.7551676035\n",
      "After 240 epochs, train loss 1.7471909523\n",
      "After 250 epochs, train loss 1.7406009436\n",
      "After 260 epochs, train loss 1.7351504564\n",
      "After 270 epochs, train loss 1.7306388617\n",
      "After 280 epochs, train loss 1.7269021273\n",
      "After 290 epochs, train loss 1.7238059044\n",
      "After 300 epochs, train loss 1.7212393284\n",
      "After 310 epochs, train loss 1.7191115618\n",
      "After 320 epochs, train loss 1.7173475027\n",
      "After 330 epochs, train loss 1.7158850431\n",
      "After 340 epochs, train loss 1.7146723270\n",
      "After 350 epochs, train loss 1.7136667967\n",
      "After 360 epochs, train loss 1.7128335238\n",
      "After 370 epochs, train loss 1.7121428251\n",
      "After 380 epochs, train loss 1.7115702629\n",
      "After 390 epochs, train loss 1.7110961676\n",
      "After 400 epochs, train loss 1.7107031345\n",
      "After 410 epochs, train loss 1.7103776932\n",
      "After 420 epochs, train loss 1.7101082802\n",
      "After 430 epochs, train loss 1.7098853588\n",
      "After 440 epochs, train loss 1.7097005844\n",
      "After 450 epochs, train loss 1.7095479965\n",
      "After 460 epochs, train loss 1.7094213963\n",
      "After 470 epochs, train loss 1.7093169689\n",
      "After 480 epochs, train loss 1.7092304230\n",
      "After 490 epochs, train loss 1.7091588974\n",
      "Final RMSE: 1.3073\n",
      "Recall: tensor(0.0007)\n",
      "Pass 1\n",
      "After 0 epochs, train loss 225.1354827881\n",
      "After 10 epochs, train loss 80.6645965576\n",
      "After 20 epochs, train loss 30.5131874084\n",
      "After 30 epochs, train loss 13.7205915451\n",
      "After 40 epochs, train loss 7.6463851929\n",
      "After 50 epochs, train loss 5.1237673759\n",
      "After 60 epochs, train loss 3.9044139385\n",
      "After 70 epochs, train loss 3.2294371128\n",
      "After 80 epochs, train loss 2.8138580322\n",
      "After 90 epochs, train loss 2.5374622345\n",
      "After 100 epochs, train loss 2.3433871269\n",
      "After 110 epochs, train loss 2.2017805576\n",
      "After 120 epochs, train loss 2.0955307484\n",
      "After 130 epochs, train loss 2.0141201019\n",
      "After 140 epochs, train loss 1.9507223368\n",
      "After 150 epochs, train loss 1.9007182121\n",
      "After 160 epochs, train loss 1.8608744144\n",
      "After 170 epochs, train loss 1.8288654089\n",
      "After 180 epochs, train loss 1.8029803038\n",
      "After 190 epochs, train loss 1.7819352150\n",
      "After 200 epochs, train loss 1.7647509575\n",
      "After 210 epochs, train loss 1.7506707907\n",
      "After 220 epochs, train loss 1.7391012907\n",
      "After 230 epochs, train loss 1.7295736074\n",
      "After 240 epochs, train loss 1.7217133045\n",
      "After 250 epochs, train loss 1.7152197361\n",
      "After 260 epochs, train loss 1.7098492384\n",
      "After 270 epochs, train loss 1.7054040432\n",
      "After 280 epochs, train loss 1.7017225027\n",
      "After 290 epochs, train loss 1.6986716986\n",
      "After 300 epochs, train loss 1.6961432695\n",
      "After 310 epochs, train loss 1.6940468550\n",
      "After 320 epochs, train loss 1.6923087835\n",
      "After 330 epochs, train loss 1.6908676624\n",
      "After 340 epochs, train loss 1.6896729469\n",
      "After 350 epochs, train loss 1.6886823177\n",
      "After 360 epochs, train loss 1.6878612041\n",
      "After 370 epochs, train loss 1.6871805191\n",
      "After 380 epochs, train loss 1.6866165400\n",
      "After 390 epochs, train loss 1.6861492395\n",
      "After 400 epochs, train loss 1.6857621670\n",
      "After 410 epochs, train loss 1.6854416132\n",
      "After 420 epochs, train loss 1.6851760149\n",
      "After 430 epochs, train loss 1.6849561930\n",
      "After 440 epochs, train loss 1.6847745180\n",
      "After 450 epochs, train loss 1.6846235991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 460 epochs, train loss 1.6844992638\n",
      "After 470 epochs, train loss 1.6843963861\n",
      "After 480 epochs, train loss 1.6843110323\n",
      "After 490 epochs, train loss 1.6842405796\n",
      "Final RMSE: 1.2978\n",
      "Recall: tensor(0.0002)\n",
      "Pass 2\n",
      "After 0 epochs, train loss 225.5435485840\n",
      "After 10 epochs, train loss 80.8747177124\n",
      "After 20 epochs, train loss 30.6253719330\n",
      "After 30 epochs, train loss 13.7871551514\n",
      "After 40 epochs, train loss 7.6914353371\n",
      "After 50 epochs, train loss 5.1579160690\n",
      "After 60 epochs, train loss 3.9324758053\n",
      "After 70 epochs, train loss 3.2537624836\n",
      "After 80 epochs, train loss 2.8357043266\n",
      "After 90 epochs, train loss 2.5575656891\n",
      "After 100 epochs, train loss 2.3622136116\n",
      "After 110 epochs, train loss 2.2196433544\n",
      "After 120 epochs, train loss 2.1126503944\n",
      "After 130 epochs, train loss 2.0306563377\n",
      "After 140 epochs, train loss 1.9667959213\n",
      "After 150 epochs, train loss 1.9164209366\n",
      "After 160 epochs, train loss 1.8762774467\n",
      "After 170 epochs, train loss 1.8440253735\n",
      "After 180 epochs, train loss 1.8179415464\n",
      "After 190 epochs, train loss 1.7967339754\n",
      "After 200 epochs, train loss 1.7794163227\n",
      "After 210 epochs, train loss 1.7652262449\n",
      "After 220 epochs, train loss 1.7535657883\n",
      "After 230 epochs, train loss 1.7439631224\n",
      "After 240 epochs, train loss 1.7360409498\n",
      "After 250 epochs, train loss 1.7294962406\n",
      "After 260 epochs, train loss 1.7240834236\n",
      "After 270 epochs, train loss 1.7196030617\n",
      "After 280 epochs, train loss 1.7158921957\n",
      "After 290 epochs, train loss 1.7128174305\n",
      "After 300 epochs, train loss 1.7102687359\n",
      "After 310 epochs, train loss 1.7081558704\n",
      "After 320 epochs, train loss 1.7064042091\n",
      "After 330 epochs, train loss 1.7049518824\n",
      "After 340 epochs, train loss 1.7037475109\n",
      "After 350 epochs, train loss 1.7027492523\n",
      "After 360 epochs, train loss 1.7019214630\n",
      "After 370 epochs, train loss 1.7012356520\n",
      "After 380 epochs, train loss 1.7006673813\n",
      "After 390 epochs, train loss 1.7001961470\n",
      "After 400 epochs, train loss 1.6998062134\n",
      "After 410 epochs, train loss 1.6994830370\n",
      "After 420 epochs, train loss 1.6992155313\n",
      "After 430 epochs, train loss 1.6989939213\n",
      "After 440 epochs, train loss 1.6988106966\n",
      "After 450 epochs, train loss 1.6986588240\n",
      "After 460 epochs, train loss 1.6985334158\n",
      "After 470 epochs, train loss 1.6984295845\n",
      "After 480 epochs, train loss 1.6983435154\n",
      "After 490 epochs, train loss 1.6982727051\n",
      "Final RMSE: 1.3032\n",
      "Recall: tensor(0.0004)\n",
      "Pass 3\n",
      "After 0 epochs, train loss 227.5089263916\n",
      "After 10 epochs, train loss 81.8208312988\n",
      "After 20 epochs, train loss 31.0840358734\n",
      "After 30 epochs, train loss 14.0273027420\n",
      "After 40 epochs, train loss 7.8323745728\n",
      "After 50 epochs, train loss 5.2504668236\n",
      "After 60 epochs, train loss 3.9990761280\n",
      "After 70 epochs, train loss 3.3050992489\n",
      "After 80 epochs, train loss 2.8773405552\n",
      "After 90 epochs, train loss 2.5926568508\n",
      "After 100 epochs, train loss 2.3926863670\n",
      "After 110 epochs, train loss 2.2467458248\n",
      "After 120 epochs, train loss 2.1372296810\n",
      "After 130 epochs, train loss 2.0533089638\n",
      "After 140 epochs, train loss 1.9879533052\n",
      "After 150 epochs, train loss 1.9364031553\n",
      "After 160 epochs, train loss 1.8953266144\n",
      "After 170 epochs, train loss 1.8623272181\n",
      "After 180 epochs, train loss 1.8356407881\n",
      "After 190 epochs, train loss 1.8139442205\n",
      "After 200 epochs, train loss 1.7962281704\n",
      "After 210 epochs, train loss 1.7817121744\n",
      "After 220 epochs, train loss 1.7697846889\n",
      "After 230 epochs, train loss 1.7599620819\n",
      "After 240 epochs, train loss 1.7518588305\n",
      "After 250 epochs, train loss 1.7451643944\n",
      "After 260 epochs, train loss 1.7396278381\n",
      "After 270 epochs, train loss 1.7350451946\n",
      "After 280 epochs, train loss 1.7312499285\n",
      "After 290 epochs, train loss 1.7281050682\n",
      "After 300 epochs, train loss 1.7254984379\n",
      "After 310 epochs, train loss 1.7233374119\n",
      "After 320 epochs, train loss 1.7215456963\n",
      "After 330 epochs, train loss 1.7200602293\n",
      "After 340 epochs, train loss 1.7188285589\n",
      "After 350 epochs, train loss 1.7178072929\n",
      "After 360 epochs, train loss 1.7169609070\n",
      "After 370 epochs, train loss 1.7162594795\n",
      "After 380 epochs, train loss 1.7156779766\n",
      "After 390 epochs, train loss 1.7151962519\n",
      "After 400 epochs, train loss 1.7147971392\n",
      "After 410 epochs, train loss 1.7144668102\n",
      "After 420 epochs, train loss 1.7141932249\n",
      "After 430 epochs, train loss 1.7139666080\n",
      "After 440 epochs, train loss 1.7137790918\n",
      "After 450 epochs, train loss 1.7136237621\n",
      "After 460 epochs, train loss 1.7134953737\n",
      "After 470 epochs, train loss 1.7133892775\n",
      "After 480 epochs, train loss 1.7133013010\n",
      "After 490 epochs, train loss 1.7132285833\n",
      "Final RMSE: 1.3089\n",
      "Recall: tensor(0.0008)\n",
      "Pass 4\n",
      "After 0 epochs, train loss 225.8280029297\n",
      "After 10 epochs, train loss 81.0225067139\n",
      "After 20 epochs, train loss 30.7045860291\n",
      "After 30 epochs, train loss 13.8337955475\n",
      "After 40 epochs, train loss 7.7222967148\n",
      "After 50 epochs, train loss 5.1805205345\n",
      "After 60 epochs, train loss 3.9503269196\n",
      "After 70 epochs, train loss 3.2686343193\n",
      "After 80 epochs, train loss 2.8485791683\n",
      "After 90 epochs, train loss 2.5690343380\n",
      "After 100 epochs, train loss 2.3726551533\n",
      "After 110 epochs, train loss 2.2293143272\n",
      "After 120 epochs, train loss 2.1217308044\n",
      "After 130 epochs, train loss 2.0392775536\n",
      "After 140 epochs, train loss 1.9750549793\n",
      "After 150 epochs, train loss 1.9243915081\n",
      "After 160 epochs, train loss 1.8840165138\n",
      "After 170 epochs, train loss 1.8515774012\n",
      "After 180 epochs, train loss 1.8253413439\n",
      "After 190 epochs, train loss 1.8040094376\n",
      "After 200 epochs, train loss 1.7865900993\n",
      "After 210 epochs, train loss 1.7723162174\n",
      "After 220 epochs, train loss 1.7605869770\n",
      "After 230 epochs, train loss 1.7509276867\n",
      "After 240 epochs, train loss 1.7429584265\n",
      "After 250 epochs, train loss 1.7363746166\n",
      "After 260 epochs, train loss 1.7309296131\n",
      "After 270 epochs, train loss 1.7264227867\n",
      "After 280 epochs, train loss 1.7226898670\n",
      "After 290 epochs, train loss 1.7195965052\n",
      "After 300 epochs, train loss 1.7170326710\n",
      "After 310 epochs, train loss 1.7149075270\n",
      "After 320 epochs, train loss 1.7131452560\n",
      "After 330 epochs, train loss 1.7116841078\n",
      "After 340 epochs, train loss 1.7104725838\n",
      "After 350 epochs, train loss 1.7094682455\n",
      "After 360 epochs, train loss 1.7086356878\n",
      "After 370 epochs, train loss 1.7079455853\n",
      "After 380 epochs, train loss 1.7073738575\n",
      "After 390 epochs, train loss 1.7069001198\n",
      "After 400 epochs, train loss 1.7065075636\n",
      "After 410 epochs, train loss 1.7061824799\n",
      "After 420 epochs, train loss 1.7059131861\n",
      "After 430 epochs, train loss 1.7056905031\n",
      "After 440 epochs, train loss 1.7055060863\n",
      "After 450 epochs, train loss 1.7053532600\n",
      "After 460 epochs, train loss 1.7052270174\n",
      "After 470 epochs, train loss 1.7051227093\n",
      "After 480 epochs, train loss 1.7050361633\n",
      "After 490 epochs, train loss 1.7049647570\n",
      "Final RMSE: 1.3057\n",
      "Recall: tensor(0.0003)\n",
      "Pass 0\n",
      "After 0 epochs, train loss 451.6412048340\n",
      "After 10 epochs, train loss 173.7112579346\n",
      "After 20 epochs, train loss 71.7448425293\n",
      "After 30 epochs, train loss 34.8723220825\n",
      "After 40 epochs, train loss 20.3500938416\n",
      "After 50 epochs, train loss 13.8228969574\n",
      "After 60 epochs, train loss 10.4594907761\n",
      "After 70 epochs, train loss 8.5089006424\n",
      "After 80 epochs, train loss 7.2692160606\n",
      "After 90 epochs, train loss 6.4271330833\n",
      "After 100 epochs, train loss 5.8273839951\n",
      "After 110 epochs, train loss 5.3854203224\n",
      "After 120 epochs, train loss 5.0514240265\n",
      "After 130 epochs, train loss 4.7941341400\n",
      "After 140 epochs, train loss 4.5929522514\n",
      "After 150 epochs, train loss 4.4337682724\n",
      "After 160 epochs, train loss 4.3066139221\n",
      "After 170 epochs, train loss 4.2042632103\n",
      "After 180 epochs, train loss 4.1213660240\n",
      "After 190 epochs, train loss 4.0538868904\n",
      "After 200 epochs, train loss 3.9987349510\n",
      "After 210 epochs, train loss 3.9535100460\n",
      "After 220 epochs, train loss 3.9163284302\n",
      "After 230 epochs, train loss 3.8856949806\n",
      "After 240 epochs, train loss 3.8604135513\n",
      "After 250 epochs, train loss 3.8395230770\n",
      "After 260 epochs, train loss 3.8222427368\n",
      "After 270 epochs, train loss 3.8079376221\n",
      "After 280 epochs, train loss 3.7960886955\n",
      "After 290 epochs, train loss 3.7862706184\n",
      "After 300 epochs, train loss 3.7781331539\n",
      "After 310 epochs, train loss 3.7713871002\n",
      "After 320 epochs, train loss 3.7657942772\n",
      "After 330 epochs, train loss 3.7611572742\n",
      "After 340 epochs, train loss 3.7573130131\n",
      "After 350 epochs, train loss 3.7541263103\n",
      "After 360 epochs, train loss 3.7514851093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 370 epochs, train loss 3.7492964268\n",
      "After 380 epochs, train loss 3.7474830151\n",
      "After 390 epochs, train loss 3.7459802628\n",
      "After 400 epochs, train loss 3.7447361946\n",
      "After 410 epochs, train loss 3.7437057495\n",
      "After 420 epochs, train loss 3.7428526878\n",
      "After 430 epochs, train loss 3.7421464920\n",
      "After 440 epochs, train loss 3.7415623665\n",
      "After 450 epochs, train loss 3.7410788536\n",
      "After 460 epochs, train loss 3.7406785488\n",
      "After 470 epochs, train loss 3.7403478622\n",
      "After 480 epochs, train loss 3.7400746346\n",
      "After 490 epochs, train loss 3.7398486137\n",
      "Final RMSE: 1.9338\n",
      "Recall: tensor(0.0005)\n",
      "Pass 1\n",
      "After 0 epochs, train loss 454.2557067871\n",
      "After 10 epochs, train loss 174.9907226562\n",
      "After 20 epochs, train loss 72.3816986084\n",
      "After 30 epochs, train loss 35.2170906067\n",
      "After 40 epochs, train loss 20.5597553253\n",
      "After 50 epochs, train loss 13.9652824402\n",
      "After 60 epochs, train loss 10.5650281906\n",
      "After 70 epochs, train loss 8.5923271179\n",
      "After 80 epochs, train loss 7.3383316994\n",
      "After 90 epochs, train loss 6.4864368439\n",
      "After 100 epochs, train loss 5.8796663284\n",
      "After 110 epochs, train loss 5.4325165749\n",
      "After 120 epochs, train loss 5.0945944786\n",
      "After 130 epochs, train loss 4.8342804909\n",
      "After 140 epochs, train loss 4.6307330132\n",
      "After 150 epochs, train loss 4.4696769714\n",
      "After 160 epochs, train loss 4.3410272598\n",
      "After 170 epochs, train loss 4.2374734879\n",
      "After 180 epochs, train loss 4.1536026001\n",
      "After 190 epochs, train loss 4.0853304863\n",
      "After 200 epochs, train loss 4.0295305252\n",
      "After 210 epochs, train loss 3.9837753773\n",
      "After 220 epochs, train loss 3.9461565018\n",
      "After 230 epochs, train loss 3.9151630402\n",
      "After 240 epochs, train loss 3.8895852566\n",
      "After 250 epochs, train loss 3.8684492111\n",
      "After 260 epochs, train loss 3.8509657383\n",
      "After 270 epochs, train loss 3.8364927769\n",
      "After 280 epochs, train loss 3.8245048523\n",
      "After 290 epochs, train loss 3.8145711422\n",
      "After 300 epochs, train loss 3.8063378334\n",
      "After 310 epochs, train loss 3.7995128632\n",
      "After 320 epochs, train loss 3.7938537598\n",
      "After 330 epochs, train loss 3.7891631126\n",
      "After 340 epochs, train loss 3.7852735519\n",
      "After 350 epochs, train loss 3.7820494175\n",
      "After 360 epochs, train loss 3.7793772221\n",
      "After 370 epochs, train loss 3.7771627903\n",
      "After 380 epochs, train loss 3.7753276825\n",
      "After 390 epochs, train loss 3.7738082409\n",
      "After 400 epochs, train loss 3.7725489140\n",
      "After 410 epochs, train loss 3.7715065479\n",
      "After 420 epochs, train loss 3.7706432343\n",
      "After 430 epochs, train loss 3.7699286938\n",
      "After 440 epochs, train loss 3.7693374157\n",
      "After 450 epochs, train loss 3.7688484192\n",
      "After 460 epochs, train loss 3.7684438229\n",
      "After 470 epochs, train loss 3.7681093216\n",
      "After 480 epochs, train loss 3.7678320408\n",
      "After 490 epochs, train loss 3.7676033974\n",
      "Final RMSE: 1.9410\n",
      "Recall: tensor(0.0003)\n",
      "Pass 2\n",
      "After 0 epochs, train loss 451.8643188477\n",
      "After 10 epochs, train loss 173.7679290771\n",
      "After 20 epochs, train loss 71.7354354858\n",
      "After 30 epochs, train loss 34.8417129517\n",
      "After 40 epochs, train loss 20.3147907257\n",
      "After 50 epochs, train loss 13.7880496979\n",
      "After 60 epochs, train loss 10.4263887405\n",
      "After 70 epochs, train loss 8.4776430130\n",
      "After 80 epochs, train loss 7.2395868301\n",
      "After 90 epochs, train loss 6.3988628387\n",
      "After 100 epochs, train loss 5.8002276421\n",
      "After 110 epochs, train loss 5.3591747284\n",
      "After 120 epochs, train loss 5.0259203911\n",
      "After 130 epochs, train loss 4.7692384720\n",
      "After 140 epochs, train loss 4.5685548782\n",
      "After 150 epochs, train loss 4.4097809792\n",
      "After 160 epochs, train loss 4.2829642296\n",
      "After 170 epochs, train loss 4.1808924675\n",
      "After 180 epochs, train loss 4.0982255936\n",
      "After 190 epochs, train loss 4.0309371948\n",
      "After 200 epochs, train loss 3.9759440422\n",
      "After 210 epochs, train loss 3.9308495522\n",
      "After 220 epochs, train loss 3.8937766552\n",
      "After 230 epochs, train loss 3.8632318974\n",
      "After 240 epochs, train loss 3.8380255699\n",
      "After 250 epochs, train loss 3.8171963692\n",
      "After 260 epochs, train loss 3.7999670506\n",
      "After 270 epochs, train loss 3.7857043743\n",
      "After 280 epochs, train loss 3.7738904953\n",
      "After 290 epochs, train loss 3.7641015053\n",
      "After 300 epochs, train loss 3.7559878826\n",
      "After 310 epochs, train loss 3.7492616177\n",
      "After 320 epochs, train loss 3.7436847687\n",
      "After 330 epochs, train loss 3.7390620708\n",
      "After 340 epochs, train loss 3.7352294922\n",
      "After 350 epochs, train loss 3.7320518494\n",
      "After 360 epochs, train loss 3.7294182777\n",
      "After 370 epochs, train loss 3.7272360325\n",
      "After 380 epochs, train loss 3.7254276276\n",
      "After 390 epochs, train loss 3.7239291668\n",
      "After 400 epochs, train loss 3.7226886749\n",
      "After 410 epochs, train loss 3.7216610909\n",
      "After 420 epochs, train loss 3.7208104134\n",
      "After 430 epochs, train loss 3.7201061249\n",
      "After 440 epochs, train loss 3.7195234299\n",
      "After 450 epochs, train loss 3.7190415859\n",
      "After 460 epochs, train loss 3.7186424732\n",
      "After 470 epochs, train loss 3.7183127403\n",
      "After 480 epochs, train loss 3.7180399895\n",
      "After 490 epochs, train loss 3.7178144455\n",
      "Final RMSE: 1.9281\n",
      "Recall: tensor(0.0011)\n",
      "Pass 3\n",
      "After 0 epochs, train loss 450.3216247559\n",
      "After 10 epochs, train loss 172.9926147461\n",
      "After 20 epochs, train loss 71.3336639404\n",
      "After 30 epochs, train loss 34.6133346558\n",
      "After 40 epochs, train loss 20.1689338684\n",
      "After 50 epochs, train loss 13.6846990585\n",
      "After 60 epochs, train loss 10.3471384048\n",
      "After 70 epochs, train loss 8.4133205414\n",
      "After 80 epochs, train loss 7.1851859093\n",
      "After 90 epochs, train loss 6.3514151573\n",
      "After 100 epochs, train loss 5.7578454018\n",
      "After 110 epochs, train loss 5.3205871582\n",
      "After 120 epochs, train loss 4.9902391434\n",
      "After 130 epochs, train loss 4.7358183861\n",
      "After 140 epochs, train loss 4.5369186401\n",
      "After 150 epochs, train loss 4.3795652390\n",
      "After 160 epochs, train loss 4.2538895607\n",
      "After 170 epochs, train loss 4.1527404785\n",
      "After 180 epochs, train loss 4.0708231926\n",
      "After 190 epochs, train loss 4.0041465759\n",
      "After 200 epochs, train loss 3.9496536255\n",
      "After 210 epochs, train loss 3.9049720764\n",
      "After 220 epochs, train loss 3.8682377338\n",
      "After 230 epochs, train loss 3.8379733562\n",
      "After 240 epochs, train loss 3.8129975796\n",
      "After 250 epochs, train loss 3.7923598289\n",
      "After 260 epochs, train loss 3.7752881050\n",
      "After 270 epochs, train loss 3.7611565590\n",
      "After 280 epochs, train loss 3.7494516373\n",
      "After 290 epochs, train loss 3.7397525311\n",
      "After 300 epochs, train loss 3.7317132950\n",
      "After 310 epochs, train loss 3.7250483036\n",
      "After 320 epochs, train loss 3.7195231915\n",
      "After 330 epochs, train loss 3.7149424553\n",
      "After 340 epochs, train loss 3.7111449242\n",
      "After 350 epochs, train loss 3.7079963684\n",
      "After 360 epochs, train loss 3.7053868771\n",
      "After 370 epochs, train loss 3.7032246590\n",
      "After 380 epochs, train loss 3.7014329433\n",
      "After 390 epochs, train loss 3.6999487877\n",
      "After 400 epochs, train loss 3.6987197399\n",
      "After 410 epochs, train loss 3.6977012157\n",
      "After 420 epochs, train loss 3.6968584061\n",
      "After 430 epochs, train loss 3.6961607933\n",
      "After 440 epochs, train loss 3.6955833435\n",
      "After 450 epochs, train loss 3.6951057911\n",
      "After 460 epochs, train loss 3.6947104931\n",
      "After 470 epochs, train loss 3.6943833828\n",
      "After 480 epochs, train loss 3.6941134930\n",
      "After 490 epochs, train loss 3.6938896179\n",
      "Final RMSE: 1.9219\n",
      "Recall: tensor(0.0006)\n",
      "Pass 4\n",
      "After 0 epochs, train loss 455.1708068848\n",
      "After 10 epochs, train loss 175.4730377197\n",
      "After 20 epochs, train loss 72.6448287964\n",
      "After 30 epochs, train loss 35.3738250732\n",
      "After 40 epochs, train loss 20.6636009216\n",
      "After 50 epochs, train loss 14.0408725739\n",
      "After 60 epochs, train loss 10.6241626740\n",
      "After 70 epochs, train loss 8.6410884857\n",
      "After 80 epochs, train loss 7.3801250458\n",
      "After 90 epochs, train loss 6.5233182907\n",
      "After 100 epochs, train loss 5.9129567146\n",
      "After 110 epochs, train loss 5.4631109238\n",
      "After 120 epochs, train loss 5.1231222153\n",
      "After 130 epochs, train loss 4.8611969948\n",
      "After 140 epochs, train loss 4.6563782692\n",
      "After 150 epochs, train loss 4.4943099022\n",
      "After 160 epochs, train loss 4.3648467064\n",
      "After 170 epochs, train loss 4.2606348991\n",
      "After 180 epochs, train loss 4.1762280464\n",
      "After 190 epochs, train loss 4.1075191498\n",
      "After 200 epochs, train loss 4.0513610840\n",
      "After 210 epochs, train loss 4.0053105354\n",
      "After 220 epochs, train loss 3.9674496651\n",
      "After 230 epochs, train loss 3.9362561703\n",
      "After 240 epochs, train loss 3.9105131626\n",
      "After 250 epochs, train loss 3.8892400265\n",
      "After 260 epochs, train loss 3.8716440201\n",
      "After 270 epochs, train loss 3.8570775986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 280 epochs, train loss 3.8450121880\n",
      "After 290 epochs, train loss 3.8350141048\n",
      "After 300 epochs, train loss 3.8267283440\n",
      "After 310 epochs, train loss 3.8198583126\n",
      "After 320 epochs, train loss 3.8141627312\n",
      "After 330 epochs, train loss 3.8094413280\n",
      "After 340 epochs, train loss 3.8055269718\n",
      "After 350 epochs, train loss 3.8022820950\n",
      "After 360 epochs, train loss 3.7995927334\n",
      "After 370 epochs, train loss 3.7973639965\n",
      "After 380 epochs, train loss 3.7955172062\n",
      "After 390 epochs, train loss 3.7939872742\n",
      "After 400 epochs, train loss 3.7927205563\n",
      "After 410 epochs, train loss 3.7916710377\n",
      "After 420 epochs, train loss 3.7908022404\n",
      "After 430 epochs, train loss 3.7900836468\n",
      "After 440 epochs, train loss 3.7894883156\n",
      "After 450 epochs, train loss 3.7889962196\n",
      "After 460 epochs, train loss 3.7885882854\n",
      "After 470 epochs, train loss 3.7882518768\n",
      "After 480 epochs, train loss 3.7879731655\n",
      "After 490 epochs, train loss 3.7877430916\n",
      "Final RMSE: 1.9462\n",
      "Recall: tensor(0.0008)\n",
      "Pass 0\n",
      "After 0 epochs, train loss 1139.7547607422\n",
      "After 10 epochs, train loss 466.6983032227\n",
      "After 20 epochs, train loss 208.0697021484\n",
      "After 30 epochs, train loss 108.6681060791\n",
      "After 40 epochs, train loss 66.9064636230\n",
      "After 50 epochs, train loss 46.9948387146\n",
      "After 60 epochs, train loss 36.2239646912\n",
      "After 70 epochs, train loss 29.7410945892\n",
      "After 80 epochs, train loss 25.5067558289\n",
      "After 90 epochs, train loss 22.5721588135\n",
      "After 100 epochs, train loss 20.4503383636\n",
      "After 110 epochs, train loss 18.8683986664\n",
      "After 120 epochs, train loss 17.6617794037\n",
      "After 130 epochs, train loss 16.7252883911\n",
      "After 140 epochs, train loss 15.9885158539\n",
      "After 150 epochs, train loss 15.4026060104\n",
      "After 160 epochs, train loss 14.9326410294\n",
      "After 170 epochs, train loss 14.5530567169\n",
      "After 180 epochs, train loss 14.2447519302\n",
      "After 190 epochs, train loss 13.9932136536\n",
      "After 200 epochs, train loss 13.7872400284\n",
      "After 210 epochs, train loss 13.6180849075\n",
      "After 220 epochs, train loss 13.4788455963\n",
      "After 230 epochs, train loss 13.3640146255\n",
      "After 240 epochs, train loss 13.2691783905\n",
      "After 250 epochs, train loss 13.1907653809\n",
      "After 260 epochs, train loss 13.1258754730\n",
      "After 270 epochs, train loss 13.0721435547\n",
      "After 280 epochs, train loss 13.0276279449\n",
      "After 290 epochs, train loss 12.9907369614\n",
      "After 300 epochs, train loss 12.9601593018\n",
      "After 310 epochs, train loss 12.9348115921\n",
      "After 320 epochs, train loss 12.9137983322\n",
      "After 330 epochs, train loss 12.8963794708\n",
      "After 340 epochs, train loss 12.8819408417\n",
      "After 350 epochs, train loss 12.8699741364\n",
      "After 360 epochs, train loss 12.8600578308\n",
      "After 370 epochs, train loss 12.8518419266\n",
      "After 380 epochs, train loss 12.8450374603\n",
      "After 390 epochs, train loss 12.8394002914\n",
      "After 400 epochs, train loss 12.8347320557\n",
      "After 410 epochs, train loss 12.8308696747\n",
      "After 420 epochs, train loss 12.8276720047\n",
      "After 430 epochs, train loss 12.8250255585\n",
      "After 440 epochs, train loss 12.8228359222\n",
      "After 450 epochs, train loss 12.8210268021\n",
      "After 460 epochs, train loss 12.8195285797\n",
      "After 470 epochs, train loss 12.8182907104\n",
      "After 480 epochs, train loss 12.8172693253\n",
      "After 490 epochs, train loss 12.8164224625\n",
      "Final RMSE: 3.5799\n",
      "Recall: tensor(0.0013)\n",
      "Pass 1\n",
      "After 0 epochs, train loss 1144.0183105469\n",
      "After 10 epochs, train loss 468.9432067871\n",
      "After 20 epochs, train loss 209.3069305420\n",
      "After 30 epochs, train loss 109.4196853638\n",
      "After 40 epochs, train loss 67.4163513184\n",
      "After 50 epochs, train loss 47.3748855591\n",
      "After 60 epochs, train loss 36.5278167725\n",
      "After 70 epochs, train loss 29.9965076447\n",
      "After 80 epochs, train loss 25.7293453217\n",
      "After 90 epochs, train loss 22.7714214325\n",
      "After 100 epochs, train loss 20.6324214935\n",
      "After 110 epochs, train loss 19.0374927521\n",
      "After 120 epochs, train loss 17.8208618164\n",
      "After 130 epochs, train loss 16.8765277863\n",
      "After 140 epochs, train loss 16.1335430145\n",
      "After 150 epochs, train loss 15.5426626205\n",
      "After 160 epochs, train loss 15.0686941147\n",
      "After 170 epochs, train loss 14.6858634949\n",
      "After 180 epochs, train loss 14.3749141693\n",
      "After 190 epochs, train loss 14.1212120056\n",
      "After 200 epochs, train loss 13.9134635925\n",
      "After 210 epochs, train loss 13.7428503036\n",
      "After 220 epochs, train loss 13.6024045944\n",
      "After 230 epochs, train loss 13.4865837097\n",
      "After 240 epochs, train loss 13.3909244537\n",
      "After 250 epochs, train loss 13.3118333817\n",
      "After 260 epochs, train loss 13.2463827133\n",
      "After 270 epochs, train loss 13.1921854019\n",
      "After 280 epochs, train loss 13.1472835541\n",
      "After 290 epochs, train loss 13.1100740433\n",
      "After 300 epochs, train loss 13.0792331696\n",
      "After 310 epochs, train loss 13.0536661148\n",
      "After 320 epochs, train loss 13.0324716568\n",
      "After 330 epochs, train loss 13.0149021149\n",
      "After 340 epochs, train loss 13.0003385544\n",
      "After 350 epochs, train loss 12.9882688522\n",
      "After 360 epochs, train loss 12.9782667160\n",
      "After 370 epochs, train loss 12.9699792862\n",
      "After 380 epochs, train loss 12.9631156921\n",
      "After 390 epochs, train loss 12.9574317932\n",
      "After 400 epochs, train loss 12.9527244568\n",
      "After 410 epochs, train loss 12.9488267899\n",
      "After 420 epochs, train loss 12.9456024170\n",
      "After 430 epochs, train loss 12.9429340363\n",
      "After 440 epochs, train loss 12.9407262802\n",
      "After 450 epochs, train loss 12.9389009476\n",
      "After 460 epochs, train loss 12.9373922348\n",
      "After 470 epochs, train loss 12.9361410141\n",
      "After 480 epochs, train loss 12.9351110458\n",
      "After 490 epochs, train loss 12.9342565536\n",
      "Final RMSE: 3.5963\n",
      "Recall: tensor(0.0005)\n",
      "Pass 2\n",
      "After 0 epochs, train loss 1135.4989013672\n",
      "After 10 epochs, train loss 464.4748840332\n",
      "After 20 epochs, train loss 206.8569335938\n",
      "After 30 epochs, train loss 107.9407806396\n",
      "After 40 epochs, train loss 66.4198684692\n",
      "After 50 epochs, train loss 46.6369895935\n",
      "After 60 epochs, train loss 35.9413032532\n",
      "After 70 epochs, train loss 29.5059528351\n",
      "After 80 epochs, train loss 25.3036155701\n",
      "After 90 epochs, train loss 22.3916435242\n",
      "After 100 epochs, train loss 20.2863979340\n",
      "After 110 epochs, train loss 18.7169303894\n",
      "After 120 epochs, train loss 17.5198936462\n",
      "After 130 epochs, train loss 16.5908737183\n",
      "After 140 epochs, train loss 15.8600053787\n",
      "After 150 epochs, train loss 15.2788047791\n",
      "After 160 epochs, train loss 14.8126258850\n",
      "After 170 epochs, train loss 14.4361095428\n",
      "After 180 epochs, train loss 14.1302986145\n",
      "After 190 epochs, train loss 13.8807973862\n",
      "After 200 epochs, train loss 13.6764936447\n",
      "After 210 epochs, train loss 13.5087118149\n",
      "After 220 epochs, train loss 13.3705997467\n",
      "After 230 epochs, train loss 13.2567014694\n",
      "After 240 epochs, train loss 13.1626338959\n",
      "After 250 epochs, train loss 13.0848579407\n",
      "After 260 epochs, train loss 13.0204944611\n",
      "After 270 epochs, train loss 12.9671993256\n",
      "After 280 epochs, train loss 12.9230442047\n",
      "After 290 epochs, train loss 12.8864517212\n",
      "After 300 epochs, train loss 12.8561229706\n",
      "After 310 epochs, train loss 12.8309822083\n",
      "After 320 epochs, train loss 12.8101377487\n",
      "After 330 epochs, train loss 12.7928590775\n",
      "After 340 epochs, train loss 12.7785377502\n",
      "After 350 epochs, train loss 12.7666664124\n",
      "After 360 epochs, train loss 12.7568311691\n",
      "After 370 epochs, train loss 12.7486810684\n",
      "After 380 epochs, train loss 12.7419309616\n",
      "After 390 epochs, train loss 12.7363405228\n",
      "After 400 epochs, train loss 12.7317123413\n",
      "After 410 epochs, train loss 12.7278785706\n",
      "After 420 epochs, train loss 12.7247076035\n",
      "After 430 epochs, train loss 12.7220811844\n",
      "After 440 epochs, train loss 12.7199106216\n",
      "After 450 epochs, train loss 12.7181158066\n",
      "After 460 epochs, train loss 12.7166299820\n",
      "After 470 epochs, train loss 12.7154026031\n",
      "After 480 epochs, train loss 12.7143869400\n",
      "After 490 epochs, train loss 12.7135486603\n",
      "Final RMSE: 3.5655\n",
      "Recall: tensor(0.0005)\n",
      "Pass 3\n",
      "After 0 epochs, train loss 1138.6547851562\n",
      "After 10 epochs, train loss 466.1078491211\n",
      "After 20 epochs, train loss 207.7367401123\n",
      "After 30 epochs, train loss 108.4620132446\n",
      "After 40 epochs, train loss 66.7650146484\n",
      "After 50 epochs, train loss 46.8888435364\n",
      "After 60 epochs, train loss 36.1390914917\n",
      "After 70 epochs, train loss 29.6697731018\n",
      "After 80 epochs, train loss 25.4446411133\n",
      "After 90 epochs, train loss 22.5165958405\n",
      "After 100 epochs, train loss 20.3995914459\n",
      "After 110 epochs, train loss 18.8212852478\n",
      "After 120 epochs, train loss 17.6174583435\n",
      "After 130 epochs, train loss 16.6831474304\n",
      "After 140 epochs, train loss 15.9480972290\n",
      "After 150 epochs, train loss 15.3635606766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 160 epochs, train loss 14.8947000504\n",
      "After 170 epochs, train loss 14.5160064697\n",
      "After 180 epochs, train loss 14.2084321976\n",
      "After 190 epochs, train loss 13.9574861526\n",
      "After 200 epochs, train loss 13.7519989014\n",
      "After 210 epochs, train loss 13.5832443237\n",
      "After 220 epochs, train loss 13.4443340302\n",
      "After 230 epochs, train loss 13.3297729492\n",
      "After 240 epochs, train loss 13.2351589203\n",
      "After 250 epochs, train loss 13.1569318771\n",
      "After 260 epochs, train loss 13.0921945572\n",
      "After 270 epochs, train loss 13.0385894775\n",
      "After 280 epochs, train loss 12.9941787720\n",
      "After 290 epochs, train loss 12.9573755264\n",
      "After 300 epochs, train loss 12.9268693924\n",
      "After 310 epochs, train loss 12.9015827179\n",
      "After 320 epochs, train loss 12.8806152344\n",
      "After 330 epochs, train loss 12.8632373810\n",
      "After 340 epochs, train loss 12.8488340378\n",
      "After 350 epochs, train loss 12.8368949890\n",
      "After 360 epochs, train loss 12.8270006180\n",
      "After 370 epochs, train loss 12.8188066483\n",
      "After 380 epochs, train loss 12.8120155334\n",
      "After 390 epochs, train loss 12.8063936234\n",
      "After 400 epochs, train loss 12.8017368317\n",
      "After 410 epochs, train loss 12.7978830338\n",
      "After 420 epochs, train loss 12.7946910858\n",
      "After 430 epochs, train loss 12.7920503616\n",
      "After 440 epochs, train loss 12.7898683548\n",
      "After 450 epochs, train loss 12.7880611420\n",
      "After 460 epochs, train loss 12.7865676880\n",
      "After 470 epochs, train loss 12.7853326797\n",
      "After 480 epochs, train loss 12.7843132019\n",
      "After 490 epochs, train loss 12.7834701538\n",
      "Final RMSE: 3.5753\n",
      "Recall: tensor(0.0003)\n",
      "Pass 4\n",
      "After 0 epochs, train loss 1133.9841308594\n",
      "After 10 epochs, train loss 463.7266540527\n",
      "After 20 epochs, train loss 206.4779205322\n",
      "After 30 epochs, train loss 107.7315368652\n",
      "After 40 epochs, train loss 66.2905197144\n",
      "After 50 epochs, train loss 46.5481414795\n",
      "After 60 epochs, train loss 35.8749504089\n",
      "After 70 epochs, train loss 29.4532413483\n",
      "After 80 epochs, train loss 25.2597980499\n",
      "After 90 epochs, train loss 22.3539485931\n",
      "After 100 epochs, train loss 20.2531013489\n",
      "After 110 epochs, train loss 18.6868953705\n",
      "After 120 epochs, train loss 17.4923286438\n",
      "After 130 epochs, train loss 16.5652198792\n",
      "After 140 epochs, train loss 15.8358497620\n",
      "After 150 epochs, train loss 15.2558383942\n",
      "After 160 epochs, train loss 14.7906093597\n",
      "After 170 epochs, train loss 14.4148559570\n",
      "After 180 epochs, train loss 14.1096677780\n",
      "After 190 epochs, train loss 13.8606729507\n",
      "After 200 epochs, train loss 13.6567840576\n",
      "After 210 epochs, train loss 13.4893398285\n",
      "After 220 epochs, train loss 13.3515110016\n",
      "After 230 epochs, train loss 13.2378425598\n",
      "After 240 epochs, train loss 13.1439657211\n",
      "After 250 epochs, train loss 13.0663461685\n",
      "After 260 epochs, train loss 13.0021152496\n",
      "After 270 epochs, train loss 12.9489240646\n",
      "After 280 epochs, train loss 12.9048604965\n",
      "After 290 epochs, train loss 12.8683452606\n",
      "After 300 epochs, train loss 12.8380746841\n",
      "After 310 epochs, train loss 12.8129844666\n",
      "After 320 epochs, train loss 12.7921829224\n",
      "After 330 epochs, train loss 12.7749385834\n",
      "After 340 epochs, train loss 12.7606477737\n",
      "After 350 epochs, train loss 12.7488012314\n",
      "After 360 epochs, train loss 12.7389869690\n",
      "After 370 epochs, train loss 12.7308530807\n",
      "After 380 epochs, train loss 12.7241163254\n",
      "After 390 epochs, train loss 12.7185363770\n",
      "After 400 epochs, train loss 12.7139167786\n",
      "After 410 epochs, train loss 12.7100925446\n",
      "After 420 epochs, train loss 12.7069272995\n",
      "After 430 epochs, train loss 12.7043075562\n",
      "After 440 epochs, train loss 12.7021417618\n",
      "After 450 epochs, train loss 12.7003479004\n",
      "After 460 epochs, train loss 12.6988658905\n",
      "After 470 epochs, train loss 12.6976413727\n",
      "After 480 epochs, train loss 12.6966285706\n",
      "After 490 epochs, train loss 12.6957921982\n",
      "Final RMSE: 3.5630\n",
      "Recall: tensor(0.0006)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[5,\n",
       "  [tensor(0.), tensor(0.), tensor(0.0002), tensor(0.), tensor(0.)],\n",
       "  tensor(3.1877e-05)],\n",
       " [10,\n",
       "  [tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)],\n",
       "  tensor(0.)],\n",
       " [20,\n",
       "  [tensor(4.7285e-05),\n",
       "   tensor(0.),\n",
       "   tensor(0.0002),\n",
       "   tensor(6.0606e-05),\n",
       "   tensor(0.0002)],\n",
       "  tensor(9.8443e-05)],\n",
       " [30,\n",
       "  [tensor(0.0001),\n",
       "   tensor(0.0002),\n",
       "   tensor(0.0003),\n",
       "   tensor(6.0606e-05),\n",
       "   tensor(0.0003)],\n",
       "  tensor(0.0002)],\n",
       " [50,\n",
       "  [tensor(0.0004),\n",
       "   tensor(0.0003),\n",
       "   tensor(0.0004),\n",
       "   tensor(0.0003),\n",
       "   tensor(0.0007)],\n",
       "  tensor(0.0004)],\n",
       " [75,\n",
       "  [tensor(0.0007),\n",
       "   tensor(0.0002),\n",
       "   tensor(0.0004),\n",
       "   tensor(0.0008),\n",
       "   tensor(0.0003)],\n",
       "  tensor(0.0005)],\n",
       " [100,\n",
       "  [tensor(0.0005),\n",
       "   tensor(0.0003),\n",
       "   tensor(0.0011),\n",
       "   tensor(0.0006),\n",
       "   tensor(0.0008)],\n",
       "  tensor(0.0007)],\n",
       " [150,\n",
       "  [tensor(0.0013),\n",
       "   tensor(0.0005),\n",
       "   tensor(0.0005),\n",
       "   tensor(0.0003),\n",
       "   tensor(0.0006)],\n",
       "  tensor(0.0006)]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_res = grid_search([5, 10, 20, 30, 50, 75, 100, 150])\n",
    "grid_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc140d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = [grid_res[i][2] for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3766895f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(3.1877e-05),\n",
       " tensor(0.),\n",
       " tensor(9.8443e-05),\n",
       " tensor(0.0002),\n",
       " tensor(0.0004),\n",
       " tensor(0.0005),\n",
       " tensor(0.0007),\n",
       " tensor(0.0006)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "255a528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85e2052a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fe6c64a92a0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter([5, 10, 20, 30, 50, 75, 100, 150], vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e00ca119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass 0\n",
      "After 0 epochs, train loss 1137.6674804688\n",
      "After 10 epochs, train loss 465.6464538574\n",
      "After 20 epochs, train loss 207.5178527832\n",
      "After 30 epochs, train loss 108.3490600586\n",
      "After 40 epochs, train loss 66.7000961304\n",
      "After 50 epochs, train loss 46.8476905823\n",
      "After 60 epochs, train loss 36.1109924316\n",
      "After 70 epochs, train loss 29.6495342255\n",
      "After 80 epochs, train loss 25.4295082092\n",
      "After 90 epochs, train loss 22.5049686432\n",
      "After 100 epochs, train loss 20.3904743195\n",
      "After 110 epochs, train loss 18.8140182495\n",
      "After 120 epochs, train loss 17.6115913391\n",
      "After 130 epochs, train loss 16.6783561707\n",
      "After 140 epochs, train loss 15.9441471100\n",
      "After 150 epochs, train loss 15.3602752686\n",
      "After 160 epochs, train loss 14.8919439316\n",
      "After 170 epochs, train loss 14.5136775970\n",
      "After 180 epochs, train loss 14.2064456940\n",
      "After 190 epochs, train loss 13.9557809830\n",
      "Final RMSE: 3.7107\n",
      "Recall: tensor(0.0005)\n",
      "Pass 1\n",
      "After 0 epochs, train loss 1134.7322998047\n",
      "After 10 epochs, train loss 464.0886840820\n",
      "After 20 epochs, train loss 206.6524658203\n",
      "After 30 epochs, train loss 107.8203735352\n",
      "After 40 epochs, train loss 66.3402481079\n",
      "After 50 epochs, train loss 46.5791244507\n",
      "After 60 epochs, train loss 35.8962326050\n",
      "After 70 epochs, train loss 29.4690723419\n",
      "After 80 epochs, train loss 25.2723274231\n",
      "After 90 epochs, train loss 22.3643474579\n",
      "After 100 epochs, train loss 20.2620525360\n",
      "After 110 epochs, train loss 18.6948204041\n",
      "After 120 epochs, train loss 17.4995040894\n",
      "After 130 epochs, train loss 16.5718345642\n",
      "After 140 epochs, train loss 15.8420352936\n",
      "After 150 epochs, train loss 15.2616891861\n",
      "After 160 epochs, train loss 14.7961978912\n",
      "After 170 epochs, train loss 14.4202375412\n",
      "After 180 epochs, train loss 14.1148805618\n",
      "After 190 epochs, train loss 13.8657493591\n",
      "Final RMSE: 3.6987\n",
      "Recall: tensor(0.0006)\n",
      "Pass 2\n",
      "After 0 epochs, train loss 1142.8947753906\n",
      "After 10 epochs, train loss 468.3387451172\n",
      "After 20 epochs, train loss 208.9620361328\n",
      "After 30 epochs, train loss 109.2013473511\n",
      "After 40 epochs, train loss 67.2626037598\n",
      "After 50 epochs, train loss 47.2569732666\n",
      "After 60 epochs, train loss 36.4316635132\n",
      "After 70 epochs, train loss 29.9146060944\n",
      "After 80 epochs, train loss 25.6573410034\n",
      "After 90 epochs, train loss 22.7065792084\n",
      "After 100 epochs, train loss 20.5729331970\n",
      "After 110 epochs, train loss 18.9820957184\n",
      "After 120 epochs, train loss 17.7686443329\n",
      "After 130 epochs, train loss 16.8268165588\n",
      "After 140 epochs, train loss 16.0858268738\n",
      "After 150 epochs, train loss 15.4965486526\n",
      "After 160 epochs, train loss 15.0238723755\n",
      "After 170 epochs, train loss 14.6420927048\n",
      "After 180 epochs, train loss 14.3320055008\n",
      "After 190 epochs, train loss 14.0790052414\n",
      "Final RMSE: 3.7270\n",
      "Recall: tensor(0.0004)\n",
      "Pass 3\n",
      "After 0 epochs, train loss 1137.3408203125\n",
      "After 10 epochs, train loss 465.4378356934\n",
      "After 20 epochs, train loss 207.3837280273\n",
      "After 30 epochs, train loss 108.2572021484\n",
      "After 40 epochs, train loss 66.6309738159\n",
      "After 50 epochs, train loss 46.7911148071\n",
      "After 60 epochs, train loss 36.0618438721\n",
      "After 70 epochs, train loss 29.6051273346\n",
      "After 80 epochs, train loss 25.3883514404\n",
      "After 90 epochs, train loss 22.4661560059\n",
      "After 100 epochs, train loss 20.3534259796\n",
      "After 110 epochs, train loss 18.7783298492\n",
      "After 120 epochs, train loss 17.5769710541\n",
      "After 130 epochs, train loss 16.6445922852\n",
      "After 140 epochs, train loss 15.9110708237\n",
      "After 150 epochs, train loss 15.3277578354\n",
      "After 160 epochs, train loss 14.8598833084\n",
      "After 170 epochs, train loss 14.4819917679\n",
      "After 180 epochs, train loss 14.1750698090\n",
      "After 190 epochs, train loss 13.9246587753\n",
      "Final RMSE: 3.7065\n",
      "Recall: tensor(0.0009)\n",
      "Pass 4\n",
      "After 0 epochs, train loss 1138.2861328125\n",
      "After 10 epochs, train loss 465.9622192383\n",
      "After 20 epochs, train loss 207.6863403320\n",
      "After 30 epochs, train loss 108.4490814209\n",
      "After 40 epochs, train loss 66.7674942017\n",
      "After 50 epochs, train loss 46.8982849121\n",
      "After 60 epochs, train loss 36.1520195007\n",
      "After 70 epochs, train loss 29.6845703125\n",
      "After 80 epochs, train loss 25.4605026245\n",
      "After 90 epochs, train loss 22.5330696106\n",
      "After 100 epochs, train loss 20.4164295197\n",
      "After 110 epochs, train loss 18.8383312225\n",
      "After 120 epochs, train loss 17.6346244812\n",
      "After 130 epochs, train loss 16.7003765106\n",
      "After 140 epochs, train loss 15.9653596878\n",
      "After 150 epochs, train loss 15.3808336258\n",
      "After 160 epochs, train loss 14.9119720459\n",
      "After 170 epochs, train loss 14.5332765579\n",
      "After 180 epochs, train loss 14.2256898880\n",
      "After 190 epochs, train loss 13.9747343063\n",
      "Final RMSE: 3.7132\n",
      "Recall: tensor(0.0003)\n"
     ]
    }
   ],
   "source": [
    "grid_res.append(grid_search([150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "306dad49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, [tensor(0.), tensor(0.), tensor(0.), tensor(0.), tensor(0.)], tensor(0.)],\n",
       " [10,\n",
       "  [tensor(1.1330e-06), tensor(0.), tensor(0.), tensor(8.4032e-05), tensor(0.)],\n",
       "  tensor(1.7033e-05)],\n",
       " [20,\n",
       "  [tensor(0.), tensor(0.0001), tensor(0.0002), tensor(0.), tensor(0.)],\n",
       "  tensor(5.1423e-05)],\n",
       " [30,\n",
       "  [tensor(5.7030e-05),\n",
       "   tensor(0.0004),\n",
       "   tensor(0.0002),\n",
       "   tensor(0.),\n",
       "   tensor(0.0001)],\n",
       "  tensor(0.0002)],\n",
       " [50,\n",
       "  [tensor(0.0005),\n",
       "   tensor(0.0007),\n",
       "   tensor(0.0006),\n",
       "   tensor(0.0003),\n",
       "   tensor(0.0010)],\n",
       "  tensor(0.0006)],\n",
       " [75,\n",
       "  [tensor(0.0006),\n",
       "   tensor(0.0004),\n",
       "   tensor(0.0008),\n",
       "   tensor(0.0009),\n",
       "   tensor(0.0005)],\n",
       "  tensor(0.0006)],\n",
       " [100,\n",
       "  [tensor(0.0003),\n",
       "   tensor(0.0004),\n",
       "   tensor(0.0007),\n",
       "   tensor(0.0017),\n",
       "   tensor(0.0006)],\n",
       "  tensor(0.0007)],\n",
       " [[150,\n",
       "   [tensor(0.0005),\n",
       "    tensor(0.0006),\n",
       "    tensor(0.0004),\n",
       "    tensor(0.0009),\n",
       "    tensor(0.0003)],\n",
       "   tensor(0.0005)]]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d99c5c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = [grid_res[i][2] for i in range(7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a4ad2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals.append(grid_res[7][0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "176ff02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.),\n",
       " tensor(1.7033e-05),\n",
       " tensor(5.1423e-05),\n",
       " tensor(0.0002),\n",
       " tensor(0.0006),\n",
       " tensor(0.0006),\n",
       " tensor(0.0007),\n",
       " tensor(0.0005)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "564a7ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fe825c09b10>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter([5, 10, 20, 30, 50, 75, 100, 150], vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fa3f49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
