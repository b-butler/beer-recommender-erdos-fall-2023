{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eba9ec1-db3e-4089-a7ee-f930e04509f7",
   "metadata": {},
   "source": [
    "# Model Training and Evalution\n",
    "\n",
    "## LightFM Model\n",
    "\n",
    "`lightfm` is an advanced matrix factorization library for\n",
    "recommender systems. We are using it for its logistic loss\n",
    "feature which has good performance on $[-1, 1]$ interaction\n",
    "matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3952ca9-839d-4818-9b1f-118d845bd681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import lightfm\n",
    "import lightfm.evaluation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import sklearn.model_selection\n",
    "\n",
    "import clean\n",
    "import eval\n",
    "import process\n",
    "import train_test_split\n",
    "\n",
    "N_THREADS = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d2bf4c-30c2-4d41-9f7d-ecbfdfdbb68a",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "\n",
    "Here we remove duplicate reviews, merge identical breweries,\n",
    "and other clean up operations before getting the final\n",
    "interaction matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9816e6e9-4a0b-4906-8538-ccff140ba3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_parquet(\"raw-data.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "734011e5-1895-4509-a396-f9938b39d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = clean.merge_similar_name_breweries(raw_df)\n",
    "cleaned_df = clean.merge_brewery_ids(cleaned_df)\n",
    "cleaned_df = clean.remove_dup_beer_rows(cleaned_df)\n",
    "cleaned_df = clean.remove_null_rows(cleaned_df)\n",
    "cleaned_df = clean.remove_duplicate_reviews(cleaned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be040b6-d895-4d4e-8928-0f2f76594c7f",
   "metadata": {},
   "source": [
    "### Processing\n",
    "\n",
    "We now get our training and testing split as well as\n",
    "define functions to help us to optimize over hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e09aeba3-7561-412e-97d9-e17fc1518cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_matrix_trans = process.InteractionMatrixTransformer(cleaned_df)\n",
    "# TODO: create pipeline that allows optimization of the creation\n",
    "# of the interaction matrix.\n",
    "matrix = int_matrix_trans.to_positive_negative(threshold=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e5cf324-8d4c-49ee-b349-30fec8f9e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = train_test_split.get_splits(matrix, 4565456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d7b3ee-71dd-445f-be28-7255174e77d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1): 119,565 (1): 1,446,939\n"
     ]
    }
   ],
   "source": [
    "a = np.unique(matrix.data, return_counts=True)\n",
    "print(*(f\"({value}): {count:,}\" for value, count in zip(*a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e78c6d6c-62a6-4cd6-b997-d061e7dc4762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coo_submatrix(arr, i):\n",
    "    return sp.sparse.coo_matrix(\n",
    "        (arr.data[i], (arr.row[i], arr.col[i])),\n",
    "        shape=arr.shape)\n",
    "    \n",
    "def score_model(estimator, X, X_train=None):\n",
    "    \"\"\"Get the negative MSE.\"\"\"\n",
    "    \n",
    "    positive_X = coo_submatrix(X, X.data == 1)\n",
    "    return eval.recall_at_k(estimator, positive_X, X_train)\n",
    "    \n",
    "def apply_split(matrix, split):\n",
    "    data = np.asarray(matrix[split[:, 0], split[:, 1]]).squeeze()\n",
    "    return sp.sparse.coo_matrix((data, (split[:, 0], split[:, 1])), shape=matrix.shape)\n",
    "\n",
    "def grid_iter(grid_spec):\n",
    "    for values in itertools.product(*grid_spec.values()):\n",
    "        yield dict(zip(grid_spec.keys(), values))\n",
    "\n",
    "def _validate_one(estimator, matrix_gen, splits, param, matrix_param):\n",
    "    estimator.set_params(**param)\n",
    "    X = matrix_gen.fit(**matrix_param)\n",
    "    scores = []\n",
    "    for train, validate in splits:\n",
    "        train_matrix = apply_split(X, train)\n",
    "        estimator.fit(train_matrix, num_threads=2)\n",
    "        score = score_model(estimator, apply_split(X, validate), train_matrix)\n",
    "        scores.append(score)\n",
    "    return {**param, **matrix_param}, np.mean(scores)\n",
    "    \n",
    "def cross_validate(estimator, matrix_gen, parameters, matrix_parameters, splits):\n",
    "    models = []\n",
    "    scores = []\n",
    "\n",
    "    parallel = joblib.Parallel(n_jobs=N_THREADS, mmap_mode=\"c\")\n",
    "    results = parallel(\n",
    "        joblib.delayed(_validate_one)(\n",
    "            estimator, matrix_gen, splits, param, mparam)\n",
    "            for param in grid_iter(parameters)\n",
    "            for mparam in grid_iter(matrix_parameters)\n",
    "    )\n",
    "    for model, score in results:\n",
    "        models.append(model)\n",
    "        scores.append(score)\n",
    "    return scores, models\n",
    "\n",
    "def save_model(filename, scores, parameters):\n",
    "    with open(filename, \"w\") as fh:\n",
    "        for score, param in zip(scores, parameters):\n",
    "            fh.write(f\"{score}: {str(param)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec1082-f5e9-4d03-8730-e4f52d9ccd90",
   "metadata": {},
   "source": [
    "### Fit and Evaluate\n",
    "\n",
    "We fit our model and evaluate its performance.\n",
    "\n",
    "### Logistic loss\n",
    "\n",
    "First we check the performance of the logistic loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf33d7f4-c9ed-4031-b056-91a3d26bb5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"no_components\": np.arange(10, 31, 2)}\n",
    "matrix_params = {\"method\": [\"positive_negative\"], \"threshold\": [2.0, 2.5, 3.0, 3.5]}\n",
    "estimator = lightfm.LightFM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f0901c-99de-40a2-ad18-7c300487beab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"logistic.data\"\n",
    "if not Path(fn).exists():\n",
    "    scores, models = cross_validate(\n",
    "        estimator,\n",
    "        int_matrix_trans,\n",
    "        params,\n",
    "        matrix_params,\n",
    "        splits)\n",
    "    save_model(fn, scores, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fb5701-cd4a-42df-844f-382aa746d6be",
   "metadata": {},
   "source": [
    "### Look at $k$OS-WARP loss\n",
    "\n",
    "Check the performance of the $k$OS-WARP loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51c5cb70-d39a-4d31-ad2a-40339176d013",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"no_components\": np.arange(10, 31, 2), \"loss\": [\"warp-kos\"], \"k\": [1, 2, 3, 4, 5]}\n",
    "matrix_params = {\"method\": [\"zero_one\"], \"threshold\": [1.5, 2.0, 2.5, 3.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3dbb559-f5fd-479b-a55d-80cc443acee1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cross_validate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarp-kos.data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(fn)\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m----> 3\u001b[0m     scores, models \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m(\n\u001b[1;32m      4\u001b[0m         estimator,\n\u001b[1;32m      5\u001b[0m         int_matrix_trans,\n\u001b[1;32m      6\u001b[0m         params,\n\u001b[1;32m      7\u001b[0m         matrix_params,\n\u001b[1;32m      8\u001b[0m         splits)\n\u001b[1;32m      9\u001b[0m     save_model(fn, scores, models)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cross_validate' is not defined"
     ]
    }
   ],
   "source": [
    "fn = \"warp-kos.data\"\n",
    "if not Path(fn).exists():\n",
    "    scores, models = cross_validate(\n",
    "        estimator,\n",
    "        int_matrix_trans,\n",
    "        params,\n",
    "        matrix_params,\n",
    "        splits)\n",
    "    save_model(fn, scores, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78183154-d6d2-4214-b721-f33b5e14cf99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       " array([30637,  1642,   504,   234,   147,    83,    37,    29,    22,\n",
       "           18,    10]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(hits, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ce2eed-90d1-4d45-adc7-c859726087cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
